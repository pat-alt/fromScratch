# Regularization

## Bias-variance tradeoff {#reg-bias}

```{r}
library(fromScratchR)
# Generate data: ----
n <- 25
p <- 24
a <- 0
b <- 1
x <- seq(a,b,length.out = n)
y <- sinusoidal(x)
v <- 0.3
```

To set the stage for the remainder of this note we will briefly revisit the bias-variance trade-off in this section. In particular we will illustrate the effect of varying the sample size $n$. Readers familiar with this topic may choose to skip this section.

As as in @bishop2006pattern we consider synthetic data generated by the sinusoidal function $f(x)=\sin(2\pi x)$. To simulate random samples of $\mathbf{y}$ we sample $n$ input values from $\mathbf{X} \sim \text{unif}(0,1)$ and introduce a random noise component $\varepsilon \sim \mathcal{N}(0,`r v`)$. Figure \@ref(fig:p-sim) shows $\mathbf{y}$ along with random draws $\mathbf{y}^*_n$.

```{r p-sim, fig.cap="Sinusoidal function and random draws."}
# True data: ----
library(ggplot2)
library(data.table)
dt_true <- data.table(y,x)
pl <- ggplot(data=dt_true, aes(x=x, y=y)) +
  geom_line()
# Simulate data: ----
n_draws <- 3
dt_star <- rbindlist(
  lapply(
    1:n_draws,
    function(x) {
      simulated <- sim_sinusoidal(n=n, sigma = v)
      data.table(y=simulated$y_star, x=simulated$x_star, n=1:n, draw=x)
    }
  )
)
pl +
  geom_point(
    data = dt_star,
    aes(x=x, y=y, colour=factor(draw))
  ) +
  scale_colour_discrete(
    name="Draw:"
  )
```

```{r param-setup}
lambda <- c(
  exp(2.6),
  exp(-0.31),
  exp(-2.4)
)
s <- 0.1
n_draws <- 100
mu <- seq(a,b,length.out = p)
```

Following @bishop2006pattern we will use a Gaussian linear model with Gaussian kernels $\exp(-\frac{(x_k-\mu_p)^{2}}{2s^2})$ as 

$$
\begin{equation} 
\begin{aligned}
&& \mathbf{y}|\mathbf{X}& =f(x) \sim \mathcal{N} \left( \sum_{j=0}^{p-1} \phi_j(x)\beta_j, v \mathbb{I}_p \right) \\
\end{aligned}
(\#eq:model)
\end{equation}
$$

with $v=`r v`$ to estimate $\hat{\mathbf{y}}_k$ from random draws $\mathbf{X}_k$. We fix the number of kernels $p=`r p`$ (and hence the number of features $M=p+1=`r p+1`$) as well as the spatial scale $s=`r s`$. To vary the complexity of the model we use a form of regularized least-squares (*Ridge regression*) and let the regularization parameter $\lambda$ vary

$$
\begin{equation} 
\begin{aligned}
&& \hat\beta&=(\lambda I + \Phi^T \Phi)^{-1}\Phi^Ty \\
\end{aligned}
(\#eq:reg-ls)
\end{equation}
$$

where high values of $\lambda$ in \@ref(eq:reg-ls) shrink parameter values towards zero. (Note that a choice $\lambda=0$ corresponds to the OLS estimator which is defined as long as $p \le n$.)

As in @bishop2006pattern we proceed as follows for each choice of $\lambda$ and each sample draw to illustrate the bias-variance trade-off:

1. Draw $N=`r n`$ time from $\mathbf{u}_k \sim \text{unif}(`r a`,`r b`)$. 
2. Let $\mathbf{X}_k^*=\mathbf{u}_k+\varepsilon_k$ with $\varepsilon \sim \mathcal{N}(0, `r v`)$.
3. Compute $\mathbf{y}_k^*=\sin(2\pi \mathbf{X}^*_k)$.
4. Extract features $\Phi_k$ from $\mathbf{X}_k^*$ and estimate the parameter vector $\beta_k^*(\Phi_k,\mathbf{y}^*_k,\lambda)$ through regularized least-squares. 
5. Predict $\hat{\mathbf{y}}_k^*=\Phi \beta_k^*$.

```{r}
Phi <- cbind(
  rep(1,n),
  sapply(
    1:length(mu),
    function(p) {
      mu_p <- mu[p]
      gauss_kernel(x=x, mu=mu_p, s = s)
    }
  )
)
dt <- rbindlist(
  lapply( # loop - draw K times
    1:n_draws,
    function(k) {
      # Draw:
      simulated <- sim_sinusoidal(n=n, sigma = v)
      y_k <- simulated$y_star
      x_k <- simulated$x_star
      rbindlist(
        lapply( # loop over regularization parameter
          1:length(lambda),
          function(t) {
            # Regularization parameter:
            lambda_t <- lambda[t]
            # Extract features:
            Phi_k <- cbind(
              rep(1,n),
              sapply(
                1:length(mu),
                function(p) {
                  mu_p <- mu[p]
                  gauss_kernel(x=x_k, mu=mu_p, s = s)
                }
              )
            )
            beta_hat <- ridge(Phi_k,y_k,lambda_t) # fit model on (y,x)
            y_hat <- c(Phi %*% beta_hat) # predict from model
            dt <- data.table(value=y_hat,draw=k,lambda=lambda_t,n=1:n,x=x)
            return(dt)
          }
        )
      )
    }
  )
)
dt[,facet_group:="single"]
dt[,colour_group:="estimates"]
# Expected values:
dt_exp = dt[,.(value=mean(value)),by=.(lambda,n,x)]
dt_exp[,facet_group:="aggregate"]
dt_exp[,colour_group:="estimates"]
dt_exp[,draw:=1] # just for aesthetics
# True values:
library(reshape)
dt_true = data.table(expand.grid.df(data.frame(value=y,x=x),data.frame(lambda=lambda)))
dt_true[,facet_group:="aggregate"]
dt_true[,colour_group:="true"]
dt_true[,draw:=2] # just for aesthetics
# Plot data:
dt_plot = rbind(
  dt,
  dt_exp,
  dt_true,
  fill=T
)
```

Applying the above procedure we can construct the familiar picture that demonstrates how increased model complexity increases variance while reducing bias (Figure \@ref(fig:plot-bias-var)). Recall that for the mean-squared error (MSE) we have 

$$
\begin{equation} 
\begin{aligned}
&& \mathbb{E} \left( (\hat{f}_n(x)-f(x))^2 \right)
&= \text{var} (\hat{f}_n(x)) + \left( \mathbb{E} \left( \hat{f}_n(x) \right) - f(x) \right)^2 \\
\end{aligned}
(\#eq:mse)
\end{equation}
$$

where the first term on the right-hand side corresponds to the variance of our prediction and the second term to its (squared) bias. In Figure \@ref(fig:plot-bias-var) as model complexity increases the variance component of the MSE increases, while the bias term diminishes. A similar pattern would have been observed if instead of using regularization we had used OLS and let the number of Gaussian kernels (and hence the number of features $p$) vary where higher values of $p$ correspond to increased model complexity.

```{r plot-bias-var, fig.cap="Bias-variance trade-off"}
dt_plot[,log_lambda := log(lambda)]
ggplot(data=dt_plot[draw<=25], aes(y=value, x=x, colour=colour_group, group=draw)) +
  geom_line() +
  facet_grid(
    rows = vars(log_lambda),
    cols = vars(facet_group)
  ) +
  scale_color_discrete(
    name="Type:"
  ) +
  labs(
    y = "f(x)"
  )
```

```{r sim-change-n}
n <- 100
m <- c(0.10, 0.5, 0.90)
x <- seq(a,b,length.out = n)
y <- sinusoidal(x)
lambda <- exp(-0.31)
Phi <- cbind(
  1,
  sapply(
    1:length(mu),
    function(p) {
      mu_p <- mu[p]
      gauss_kernel(x=x, mu=mu_p, s = s)
    }
  )
)
dt <- rbindlist(
  lapply( # loop - draw K times
    1:n_draws,
    function(k) {
      rbindlist(
        lapply( # loop over sample sizes:
          1:length(m),
          function(t) {
            n_sample <- m[t] * n
            # Draw:
            simulated <- sim_sinusoidal(n=n_sample, sigma = v)
            y_k <- simulated$y_star
            x_k <- simulated$x_star
            # Extract features:
            Phi_k <- cbind(
              1,
              sapply(
                1:length(mu),
                function(p) {
                  mu_p <- mu[p]
                  gauss_kernel(x=x_k, mu=mu_p, s = s)
                }
              )
            )
            beta_hat <- ridge(Phi_k,y_k,lambda) # fit model on (y,x)
            y_hat <- c(Phi %*% beta_hat) # predict from model
            dt <- data.table(value=y_hat,draw=k,sample_size=n_sample,n=1:n,x=x)
            return(dt)
          }
        )
      )
    }
  )
)
dt[,facet_group:="single"]
dt[,colour_group:="estimates"]
# Expected values:
dt_exp = dt[,.(value=mean(value)),by=.(sample_size,n,x)]
dt_exp[,facet_group:="aggregate"]
dt_exp[,colour_group:="estimates"]
dt_exp[,draw:=1] # just for aesthetics
# True values:
library(reshape)
dt_true = data.table(expand.grid.df(data.frame(value=y,x=x),data.frame(sample_size=m*n)))
dt_true[,facet_group:="aggregate"]
dt_true[,colour_group:="true"]
dt_true[,draw:=2] # just for aesthetics
# Plot data:
dt_plot = rbind(
  dt,
  dt_exp,
  dt_true,
  fill=T
)
```



