<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Empirical risk minimization | From Scratch</title>
  <meta name="description" content="A collection of ideas, notes, exercises and code." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Empirical risk minimization | From Scratch" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A collection of ideas, notes, exercises and code." />
  <meta name="github-repo" content="https://github.com/pat-alt/fromScratch.git" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Empirical risk minimization | From Scratch" />
  
  <meta name="twitter:description" content="A collection of ideas, notes, exercises and code." />
  

<meta name="author" content="Patrick Altmeyer" />


<meta name="date" content="2021-03-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="www/icon.ico" type="image/x-icon" />
<link rel="prev" href="class.html"/>
<link rel="next" href="regularization.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.52.2/plotly-latest.min.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.16/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#resources"><i class="fa fa-check"></i>Resources</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#r-package"><i class="fa fa-check"></i>R package</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#python-code"><i class="fa fa-check"></i>Python code</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#session-info"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
<li class="part"><span><b>I Part I</b></span></li>
<li class="chapter" data-level="1" data-path="introductory-topics.html"><a href="introductory-topics.html"><i class="fa fa-check"></i><b>1</b> Introductory topics</a></li>
<li class="chapter" data-level="2" data-path="conc.html"><a href="conc.html"><i class="fa fa-check"></i><b>2</b> Concentration inequalities</a>
<ul>
<li class="chapter" data-level="2.1" data-path="conc.html"><a href="conc.html#conc-mean"><i class="fa fa-check"></i><b>2.1</b> Empirical mean</a></li>
<li class="chapter" data-level="2.2" data-path="conc.html"><a href="conc.html#simple-non-asymptotic-concentration-inequalities"><i class="fa fa-check"></i><b>2.2</b> Simple non-asymptotic concentration inequalities</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="conc.html"><a href="conc.html#conc-markov"><i class="fa fa-check"></i><b>2.2.1</b> Markov’s inequality</a></li>
<li class="chapter" data-level="2.2.2" data-path="conc.html"><a href="conc.html#chebychevs-inequality"><i class="fa fa-check"></i><b>2.2.2</b> Chebychev’s inequality</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="conc.html"><a href="conc.html#asympotic-concentration-inequalities"><i class="fa fa-check"></i><b>2.3</b> Asympotic concentration inequalities</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="conc.html"><a href="conc.html#central-limit-theorem"><i class="fa fa-check"></i><b>2.3.1</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="conc.html"><a href="conc.html#exponential-non-asymptotic-concentration-inequalities"><i class="fa fa-check"></i><b>2.4</b> Exponential non-asymptotic concentration inequalities</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="conc.html"><a href="conc.html#chernoff-bounds"><i class="fa fa-check"></i><b>2.4.1</b> Chernoff bounds</a></li>
<li class="chapter" data-level="2.4.2" data-path="conc.html"><a href="conc.html#hoeffdings-inequality"><i class="fa fa-check"></i><b>2.4.2</b> Hoeffding’s Inequality</a></li>
<li class="chapter" data-level="2.4.3" data-path="conc.html"><a href="conc.html#bernsteins-inequality"><i class="fa fa-check"></i><b>2.4.3</b> Bernstein’s inequality</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="conc.html"><a href="conc.html#examples"><i class="fa fa-check"></i><b>2.5</b> Examples</a></li>
<li class="chapter" data-level="2.6" data-path="conc.html"><a href="conc.html#appendix"><i class="fa fa-check"></i><b>2.6</b> Appendix</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="conc.html"><a href="conc.html#example-of-a-moment-generating-function"><i class="fa fa-check"></i><b>2.6.1</b> Example of a moment generating function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="det-opt.html"><a href="det-opt.html"><i class="fa fa-check"></i><b>3</b> Optimization</a>
<ul>
<li class="chapter" data-level="3.1" data-path="det-opt.html"><a href="det-opt.html#line-search"><i class="fa fa-check"></i><b>3.1</b> Line search</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="det-opt.html"><a href="det-opt.html#methodology"><i class="fa fa-check"></i><b>3.1.1</b> Methodology</a></li>
<li class="chapter" data-level="3.1.2" data-path="det-opt.html"><a href="det-opt.html#results"><i class="fa fa-check"></i><b>3.1.2</b> Results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="regr.html"><a href="regr.html"><i class="fa fa-check"></i><b>4</b> Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="regr.html"><a href="regr.html#regr-ols"><i class="fa fa-check"></i><b>4.1</b> Ordinary least-squares</a></li>
<li class="chapter" data-level="4.2" data-path="regr.html"><a href="regr.html#regr-wls"><i class="fa fa-check"></i><b>4.2</b> Weighted least-squares</a></li>
<li class="chapter" data-level="4.3" data-path="conc.html"><a href="conc.html#appendix"><i class="fa fa-check"></i><b>4.3</b> Appendix</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="regr.html"><a href="regr.html#app-wls"><i class="fa fa-check"></i><b>4.3.1</b> Weighted least-squares</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="class.html"><a href="class.html"><i class="fa fa-check"></i><b>5</b> Classification</a>
<ul>
<li class="chapter" data-level="5.1" data-path="class.html"><a href="class.html#binary-classification"><i class="fa fa-check"></i><b>5.1</b> Binary classification</a></li>
<li class="chapter" data-level="5.2" data-path="class.html"><a href="class.html#class-knn"><i class="fa fa-check"></i><b>5.2</b> Nearest Neighbour</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="class.html"><a href="class.html#nn"><i class="fa fa-check"></i><b>5.2.1</b> 1NN</a></li>
<li class="chapter" data-level="5.2.2" data-path="class.html"><a href="class.html#knn"><i class="fa fa-check"></i><b>5.2.2</b> KNN</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="class.html"><a href="class.html#class-logit"><i class="fa fa-check"></i><b>5.3</b> Logisitic regression</a></li>
<li class="chapter" data-level="5.4" data-path="conc.html"><a href="conc.html#appendix"><i class="fa fa-check"></i><b>5.4</b> Appendix</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="class.html"><a href="class.html#irls"><i class="fa fa-check"></i><b>5.4.1</b> Iterative reweighted least-squares</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="class-emp.html"><a href="class-emp.html"><i class="fa fa-check"></i><b>6</b> Empirical risk minimization</a>
<ul>
<li class="chapter" data-level="6.1" data-path="class-emp.html"><a href="class-emp.html#excess-risk-and-overfitting-error"><i class="fa fa-check"></i><b>6.1</b> Excess risk and overfitting error</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="class-emp.html"><a href="class-emp.html#data-splitting"><i class="fa fa-check"></i><b>6.1.1</b> Data splitting</a></li>
<li class="chapter" data-level="6.1.2" data-path="class-emp.html"><a href="class-emp.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>6.1.2</b> Leave-one-out cross-validation</a></li>
<li class="chapter" data-level="6.1.3" data-path="class-emp.html"><a href="class-emp.html#reliable-case"><i class="fa fa-check"></i><b>6.1.3</b> Reliable case</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="class-emp.html"><a href="class-emp.html#rademacher-averages"><i class="fa fa-check"></i><b>6.2</b> Rademacher averages</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>7</b> Regularization</a>
<ul>
<li class="chapter" data-level="7.1" data-path="regularization.html"><a href="regularization.html#reg-bias"><i class="fa fa-check"></i><b>7.1</b> Bias-variance tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="dim-red.html"><a href="dim-red.html"><i class="fa fa-check"></i><b>8</b> Dimensionality reduction</a>
<ul>
<li class="chapter" data-level="8.1" data-path="dim-red.html"><a href="dim-red.html#random-projections"><i class="fa fa-check"></i><b>8.1</b> Random projections</a></li>
<li class="chapter" data-level="8.2" data-path="dim-red.html"><a href="dim-red.html#pca"><i class="fa fa-check"></i><b>8.2</b> PCA</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="dim-red.html"><a href="dim-red.html#the-maths-behind-pca"><i class="fa fa-check"></i><b>8.2.1</b> The maths behind PCA</a></li>
<li class="chapter" data-level="8.2.2" data-path="dim-red.html"><a href="dim-red.html#an-intuitive-example"><i class="fa fa-check"></i><b>8.2.2</b> An intuitive example</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="dim-red.html"><a href="dim-red.html#pca-for-feature-extraction"><i class="fa fa-check"></i><b>8.3</b> PCA for feature extraction</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="dim-red.html"><a href="dim-red.html#squared-elements-of-eigenvectors"><i class="fa fa-check"></i><b>8.3.1</b> Squared elements of eigenvectors</a></li>
<li class="chapter" data-level="8.3.2" data-path="dim-red.html"><a href="dim-red.html#svd"><i class="fa fa-check"></i><b>8.3.2</b> SVD</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="dim-red.html"><a href="dim-red.html#high-dimensional-data"><i class="fa fa-check"></i><b>8.4</b> High-dimensional data</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="dim-red.html"><a href="dim-red.html#regularized-svd"><i class="fa fa-check"></i><b>8.4.1</b> Regularized SVD</a></li>
<li class="chapter" data-level="8.4.2" data-path="dim-red.html"><a href="dim-red.html#fast-partial-svd"><i class="fa fa-check"></i><b>8.4.2</b> Fast, partial SVD</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="dim-red.html"><a href="dim-red.html#forward-search"><i class="fa fa-check"></i><b>8.5</b> Forward search</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="subsample.html"><a href="subsample.html"><i class="fa fa-check"></i><b>9</b> Subsampling</a>
<ul>
<li class="chapter" data-level="9.1" data-path="subsample.html"><a href="subsample.html#subsample-motivation"><i class="fa fa-check"></i><b>9.1</b> Motivation</a></li>
<li class="chapter" data-level="9.2" data-path="subsample.html"><a href="subsample.html#subsample-methods"><i class="fa fa-check"></i><b>9.2</b> Subsampling methods</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="subsample.html"><a href="subsample.html#uniform-subsampling-unif"><i class="fa fa-check"></i><b>9.2.1</b> Uniform subsampling (UNIF)</a></li>
<li class="chapter" data-level="9.2.2" data-path="subsample.html"><a href="subsample.html#basic-leveraging-blev"><i class="fa fa-check"></i><b>9.2.2</b> Basic leveraging (BLEV)</a></li>
<li class="chapter" data-level="9.2.3" data-path="subsample.html"><a href="subsample.html#predictor-length-sampling-pl"><i class="fa fa-check"></i><b>9.2.3</b> Predictor-length sampling (PL)</a></li>
<li class="chapter" data-level="9.2.4" data-path="subsample.html"><a href="subsample.html#comparison-of-methods"><i class="fa fa-check"></i><b>9.2.4</b> Comparison of methods</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="subsample.html"><a href="subsample.html#subsample-lin-reg"><i class="fa fa-check"></i><b>9.3</b> Linear regression model</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="subsample.html"><a href="subsample.html#a-review-of-zhu2015optimal"><i class="fa fa-check"></i><b>9.3.1</b> A review of <span class="citation"><span>Zhu et al.</span> (<span>2015</span>)</span></a></li>
<li class="chapter" data-level="9.3.2" data-path="subsample.html"><a href="subsample.html#computational-performance"><i class="fa fa-check"></i><b>9.3.2</b> Computational performance</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="subsample.html"><a href="subsample.html#classification-problems"><i class="fa fa-check"></i><b>9.4</b> Classification problems</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="subsample.html"><a href="subsample.html#optimal-subsampling-for-classification-problems"><i class="fa fa-check"></i><b>9.4.1</b> Optimal subsampling for classification problems</a></li>
<li class="chapter" data-level="9.4.2" data-path="subsample.html"><a href="subsample.html#class-syn"><i class="fa fa-check"></i><b>9.4.2</b> Synthetic data</a></li>
<li class="chapter" data-level="9.4.3" data-path="subsample.html"><a href="subsample.html#real-data-example"><i class="fa fa-check"></i><b>9.4.3</b> Real data example</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="subsample.html"><a href="subsample.html#concl"><i class="fa fa-check"></i><b>9.5</b> Conclusion</a></li>
<li class="chapter" data-level="9.6" data-path="conc.html"><a href="conc.html#appendix"><i class="fa fa-check"></i><b>9.6</b> Appendix</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="subsample.html"><a href="subsample.html#app-svd"><i class="fa fa-check"></i><b>9.6.1</b> From SVD to leverage scores</a></li>
<li class="chapter" data-level="9.6.2" data-path="subsample.html"><a href="subsample.html#app-pl"><i class="fa fa-check"></i><b>9.6.2</b> From optimal to prediction-length subsampling</a></li>
<li class="chapter" data-level="9.6.3" data-path="subsample.html"><a href="subsample.html#app-dens"><i class="fa fa-check"></i><b>9.6.3</b> Synthetic data</a></li>
<li class="chapter" data-level="9.6.4" data-path="subsample.html"><a href="subsample.html#app-sin"><i class="fa fa-check"></i><b>9.6.4</b> Subsampling applied to sinusoidal function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="outl.html"><a href="outl.html"><i class="fa fa-check"></i><b>10</b> Outliers</a>
<ul>
<li class="chapter" data-level="10.1" data-path="outl.html"><a href="outl.html#outl-trimmed"><i class="fa fa-check"></i><b>10.1</b> Trimmed mean estimator</a></li>
<li class="chapter" data-level="10.2" data-path="outl.html"><a href="outl.html#outl-mom"><i class="fa fa-check"></i><b>10.2</b> Median-of-means estimator</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">From Scratch</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="class-emp" class="section level1" number="6">
<h1><span class="header-section-number">Chapter 6</span> Empirical risk minimization</h1>
<div id="excess-risk-and-overfitting-error" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Excess risk and overfitting error</h2>
<p>Let <span class="math inline">\(g:\mathcal{X}\mapsto\{0,1\}\)</span> be a classifier. Given data <span class="math inline">\(D_n=((X_1,y_1),...,(X_n,y_n))\)</span> we can estimate <span class="math inline">\(R(g)=P(g(X)\ne y)\)</span> by the empirical mean <span class="math inline">\(R_n(g)= \frac{1}{n} \sum_{i=1}^{n} \mathbb{1}_{g(X_i)\ne y_i}\)</span>. Then by Hoeffding’s Inequality <a href="#thm:hoeff"><strong>??</strong></a> we have</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp; P(\left|R_n(g)-R(g)\right|\ge \varepsilon)&amp;\le 2e^{-2n\varepsilon^2} \\
\end{aligned}
\]</span>
and equivalently with probability <span class="math inline">\(\ge 1-\delta\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp; \left|R_n(g)-R(g)\right|&amp;\le \sqrt{ \frac{\log( \frac{2}{\delta})}{2n}} \\
\end{aligned}
\]</span></p>
<p>Suppose now that we have <span class="math inline">\(N\)</span> classifiers <span class="math inline">\(g^{(1)},...,g^{(n)}\)</span>. Now let <span class="math inline">\(g_n\)</span> denote the data-based classifier that minimizes the empirical risk <span class="math inline">\(R_n(g^{(j)})\)</span> among our <span class="math inline">\(N\)</span> classifiers. For its risk <span class="math inline">\(R(g_n)=P(g_n(X)\ne y|D_n)\)</span> we can establish two quantities of interest:</p>
<p>We can further establish two basic inequalities. The first one is trivial:</p>
<p><span class="math display" id="eq:overfitting-ineq">\[
\begin{equation} 
\begin{aligned}
&amp;&amp; R(g_n)-R_n(g_n)&amp;\le \max_{j=1,...,N} \left| R(g^{(j)})-R_n(g^{(j)}) \right| &amp;&amp; \text{(overfitting)} \\
\end{aligned}
\tag{6.1}
\end{equation}
\]</span></p>
<p>To derive the second one, note that we can rewrite the term in <a href="#def:exc-risk"><strong>??</strong></a> as</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp; R(g_n)-\min_{j=1,..,N}R(g^{(j)})&amp;=R(g_n)-R_n(g_n)+R_n(g_n)-\min_{j=1,..,N}R(g^{(j)}) \\
\end{aligned}
\]</span>
where <span class="math inline">\(R(g_n)-R_n(g_n)\)</span> just correspond to the overfitting error again. For the second term denote <span class="math inline">\(\bar{g}=\arg\min_{j}R(g^{(j)})\)</span> and note that</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp; R_n(g_n)-\min_{j=1,..,N}R(g^{(j)})&amp;\le R_n(\bar{g})-\min_{j=1,..,N}R(g^{(j)}) \\
\end{aligned}
\]</span></p>
<p>since by definition <span class="math inline">\(g_n\)</span> minimizes the empirical risk and hence <span class="math inline">\(R_n(g_n)\le R_n(\bar{g})\)</span>. Once again we can trivially establish that</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp; R_n(\bar{g})-\min_{j=1,..,N}R(g^{(j)})&amp;\le \max_{j=1,...,N} \left| R(g^{(j)})-R_n(g^{(j)})\right|\\
\end{aligned}
\]</span></p>
<p>which is the just the bound for the overfitting error already established in <a href="class-emp.html#eq:overfitting-ineq">(6.1)</a>. Hence, we take everything together to arrive at the second basic inequality:</p>
<p><span class="math display" id="eq:exc-risk-ineq">\[
\begin{equation} 
\begin{aligned}
&amp;&amp; R(g_n)-\min_{j=1,..,N}R(g^{(j)})&amp;\le 2\max_{j=1,...,N} \left| R(g^{(j)})-R_n(g^{(j)})\right| &amp;&amp; \text{(excess risk)} \\
\end{aligned}
\tag{6.2}
\end{equation}
\]</span></p>
<p>So both the excess risk and the overfitting error may be bounded in term of:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp; \max_{j=1,...,N} \left| R(g^{(j)})-R_n(g^{(j)})\right| \\
\end{aligned}
\]</span></p>
<p>Now let us actually derive a bound. We have</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp; P\left(\max_{j=1,...,N} \left| R(g^{(j)})-R_n(g^{(j)})\right|\ge\varepsilon\right)&amp;=P\left(\bigcup_{j=1,...,N} \left\{ \left| R(g^{(j)})-R_n(g^{(j)})\right|\ge\varepsilon\right\}\right) \\
&amp;&amp; &amp;\le \sum_{j=1}^{N}P\left(\left| R(g^{(j)})-R_n(g^{(j)})\right|\ge\varepsilon\right) \\
&amp;&amp; &amp;\le 2Ne^{-2n\varepsilon^2} \\
\end{aligned}
\]</span>
where the first inequality follows from the union bound and the second one from Hoeffding’s Inequality <a href="#thm:hoeff"><strong>??</strong></a>. Equivalently, we finally have that with probability <span class="math inline">\(\ge 1-\delta\)</span></p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp; \max_{j=1,...,N} \left| R(g^{(j)})-R_n(g^{(j)})\right|&amp;\le \sqrt{ \frac{\log ( \frac{2N}{\delta})}{2n}} \\
\end{aligned}
\]</span>
and hence the following bound:</p>
<p><span class="math display" id="eq:overfitting-bound">\[
\begin{equation} 
\begin{aligned}
&amp;&amp; R(g_n)-R_n(g_n)&amp;\le\sqrt{ \frac{\log ( \frac{2N}{\delta})}{2n}} &amp;&amp; \text{(overfitting)} \\
\end{aligned}
\tag{6.3}
\end{equation}
\]</span></p>
<p><span class="math display" id="eq:exc-risk-bound">\[
\begin{equation} 
\begin{aligned}
&amp;&amp; R(g_n)-\min_{j=1,..,N}R(g^{(j)})&amp;\le 2\sqrt{ \frac{\log ( \frac{2N}{\delta})}{2n}} &amp;&amp; \text{(excess risk)} \\
\end{aligned}
\tag{6.4}
\end{equation}
\]</span></p>
<div id="data-splitting" class="section level3" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Data splitting</h3>
<p>Let <span class="math inline">\(g_1^{(1)},...,g_n^{(N)}\)</span> be data-dependent classifiers depending on <span class="math inline">\(D_n\)</span> and suppose that an independent data set is available for testing <span class="math inline">\(D&#39;_m=((X&#39;_1,y&#39;_1),...,(X&#39;_m,y&#39;_m))\)</span>. Then we may estimate the true risk <span class="math inline">\(R(g_n^{(j)})=P(g_n^{(j)}(X)\ne y|D_n)\)</span> by the empirical risk (test error):</p>
<p><span class="math display" id="eq:test-error">\[
\begin{equation}
\begin{aligned}
&amp;&amp; R&#39;_m(g_n^{(j)})&amp;= \frac{1}{m} \sum_{i=1}^{m} \mathbb{1}_{g_n^{(j)}(X&#39;_i)\ne y&#39;_i}\\
\end{aligned}
\tag{6.5}
\end{equation}
\]</span></p>
<p>Then using the results from above, where have for the empirical risk minimizer <span class="math inline">\(g_{n,m}=\arg\min_{j=1,...,N}R&#39;_m(g_n^{(j)})\)</span> that with probability <span class="math inline">\(1-\delta\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp; R(g_{n,m})-R&#39;_m(g_{n,m})&amp;=\sqrt{ \frac{\log ( \frac{2N}{\delta})}{2m}} &amp;&amp; \text{(overfitting)} \\
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp; R(g_{n,m})-\min_{j=1,..,N}R(g_n^{(j)})&amp;\le 2\sqrt{ \frac{\log ( \frac{2N}{\delta})}{2m}} &amp;&amp; \text{(excess risk)} \\
\end{aligned}
\]</span></p>
</div>
<div id="leave-one-out-cross-validation" class="section level3" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> Leave-one-out cross-validation</h3>
<p>Instead of data-splitting (once) we can use leave-one-out cross-validation to get an estimate of the true risk of our data-based classifier. As before we have <span class="math inline">\(D_n=((X_1,y_1),...,(X_n,y_n))\)</span>. Now let <span class="math inline">\(D_{n,i}=((X_1,y_1),...,(X_{i-1},y_{i-1}),(X_{i+1},y_{i+1}),...,(X_n,y_n))\)</span> denote the subsample that contains all observations except <span class="math inline">\(i\)</span>. Then we can define:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp; R_n^{(D)}(g_n)&amp;= \frac{1}{n} \sum_{i=1}^{n} \mathbb{1}_{g_{n-1}(X_i,D_{n,i})\ne y_i} \\
\end{aligned}
\]</span></p>
<p>Now since <span class="math inline">\(g_{n-1}\)</span> does not depend on <span class="math inline">\((X_i,y_i)\)</span> we have that:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp; \mathbb{E} \left[ R_n^{(D)}(g_n) \right]&amp;= \mathbb{E} \left[ R(g_{n-1}) \right]\\
\end{aligned}
\]</span></p>
<p>But since <span class="math inline">\(\mathbb{E} \left[ R(g_{n-1}) \right]\ne\mathbb{E} \left[ R(g_{n}) \right]\)</span> we introduce a small amount of bias. In general it is not easy to establish bounds for the leave-one-out estimator, but empirically it performs well.</p>
</div>
<div id="reliable-case" class="section level3" number="6.1.3">
<h3><span class="header-section-number">6.1.3</span> Reliable case</h3>
<p>Let <span class="math inline">\(g^{(1)},...,g^{(N)}\)</span> be non-data-dependent classifiers depending on <span class="math inline">\(D_n\)</span>. Now assume one of the candidate classifier has zero risk, that is <span class="math inline">\(\min_jR(g^{(j)})=0\)</span>. We refer to this as the reliable case. Note that this implies that <span class="math inline">\(\min_jR_n(g^{(j)})=0\)</span> and also that both the excess risk and overfitting error are equal to the true risk, <span class="math inline">\(R(g_n)\)</span>. Hence, we would like to bound this quantity.</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp; P(R(g_n) \ge \varepsilon)&amp;\le P \left( \exists j\in 1,...,N: R(g^{(j)})\ge \varepsilon; R_n(g^{(j)})=0 \right)\\
&amp;&amp; &amp;\le N P \left(R(g)\ge \varepsilon \ \&amp; \ R_n(g)=0 \right)  \\
&amp;&amp; &amp;\le N(1-\varepsilon)^N \le N e^{-n\varepsilon} \\
\end{aligned}
\]</span>
where the second inequality follows from the union bound.</p>
</div>
</div>
<div id="rademacher-averages" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Rademacher averages</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="class.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regularization.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["book.epub", "EPUB"]],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true,
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
