[
["index.html", "From Scratch A bottom-up approach to data science. Preface Resources Session info", " From Scratch A bottom-up approach to data science. Patrick Altmeyer 2021-03-17 Preface This book is a collection of ideas, notes, exercises and code covering a broad range of topics from statistics, machine learning, deep learning, econometrics. The focus is on approaching problems from scratch: instead of using existing packages and libraries, we look at how exactly different methodologies can be implemented in code. This is not to undermine the value of existing packages or provide an alternative. But the bottom-up approach is educationally very rewarding: if you can write an algorithm from scratch, you have truly understood how it works. Working on it Please note that this is very much still a work-in-progress. The structure is inherently somewhat loose. Feedback and comments are therefore more than welcome, although time and resources to act on them is scarce at this point. Resources R package The book uses a complementary R package fromScratchR which collects the R code used throughout the book. The book can be read without using the package, but in case you want to play with the code you can install the development version from GitHub with: Then import the package through library(fromScratchR) Python code As for Python code, modules are currently part of the Git repository that contains the book. Session info ## R version 4.0.3 (2020-10-10) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] data.table_1.14.0 fromScratchR_0.1.0 ## ## loaded via a namespace (and not attached): ## [1] compiler_4.0.3 magrittr_2.0.1 bookdown_0.20 tools_4.0.3 ## [5] htmltools_0.5.0 rstudioapi_0.13 yaml_2.2.1 stringi_1.5.3 ## [9] rmarkdown_2.6 knitr_1.31 stringr_1.4.0 xfun_0.21 ## [13] digest_0.6.27 rlang_0.4.10 evaluate_0.14 "],
["about-the-author.html", "About the Author", " About the Author I am an economist by background with a strong interest in cross-disciplinary research on the intersection of interpretable artificial intelligence (AI), causal inference, economics and finance. Currently I am studying for the MSc Data Science at the Barcelona Graduate School of Economics. I am also affiliated with the Bank of England where I previously worked as an economist mainly focused on monetary policy briefings, research and market intelligence. "],
["introductory-topics.html", "Chapter 1 Introductory topics", " Chapter 1 Introductory topics This chapter provides a whistle-stop tour of simple, but important concepts… "],
["conc.html", "Chapter 2 Concentration inequalities 2.1 Empirical mean 2.2 Simple non-asymptotic concentration inequalities 2.3 Asympotic concentration inequalities 2.4 Exponential non-asymptotic concentration inequalities 2.5 Examples 2.6 Appendix", " Chapter 2 Concentration inequalities In order to measure the quality of an estimator we generally aim to minimize the expected value of a loss function \\(\\ell(x,y)\\). Examples include: Mean squared error (MSE) \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\ell(x,y)=(x-y)^2 &amp;\\rightarrow \\mathbb{E} \\left[ \\ell(x,y) \\right] = \\mathbb{E} (x-y)^2\\\\ \\end{aligned} \\tag{2.1} \\end{equation} \\] Mean absolute error (MAE) \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\ell(x,y)=|x-y| &amp;\\rightarrow \\mathbb{E} \\left[ \\ell(x,y) \\right] = \\mathbb{E} |x-y|\\\\ \\end{aligned} \\tag{2.2} \\end{equation} \\] More generally it is often useful to write: \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\ell(x,y)= \\mathbb{1}_{|x-y|&gt;\\varepsilon} &amp;\\rightarrow \\mathbb{E} \\left[ \\ell(x,y) \\right] = P(|x-y|&gt;\\varepsilon)\\\\ \\end{aligned} \\tag{2.3} \\end{equation} \\] 2.1 Empirical mean Let \\(m\\) denote the true value we want to estimate and \\(m_n\\) the corresponding estimator. An obvious choice for \\(m_n\\) is the empirical mean \\[ \\begin{equation} \\begin{aligned} &amp;&amp; m_n&amp;= \\frac{1}{n} \\sum_{i=1}^{n} x_i\\\\ \\end{aligned} \\tag{2.4} \\end{equation} \\] for which we have \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\mathbb{E} \\left[ m_n\\right]&amp;= \\mathbb{E} \\left[ \\frac{1}{n} \\sum_{i=1}^{n} x_i \\right] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E} \\left[x_i \\right]=m\\\\ \\end{aligned} \\tag{2.5} \\end{equation} \\] where the last equality follows from the law of large numbers. For the MSE of the empirical mean we have \\[ \\begin{aligned} &amp;&amp; \\mathbb{E} \\left[ m_n-m \\right]^2 &amp;= \\mathbb{E} \\left[ \\frac{1}{n} \\sum_{i=1}^{n} x_i-m \\right]^2\\\\ &amp;&amp; &amp;= \\mathbb{E} \\left[ \\frac{1}{n} \\sum_{i=1}^{n} (x_i-m) \\right]^2\\\\ &amp;&amp; &amp;= \\frac{1}{n^2}\\mathbb{E} \\left[ \\sum_{i=1}^{n} x_i-m \\right]^2\\\\ &amp;&amp; &amp;= \\frac{1}{n^2} n\\text{var}(x_i)\\\\ \\end{aligned} \\] and hence: \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\mathbb{E} \\left[ m_n-m \\right]^2&amp;= \\frac{\\sigma^2}{n}\\\\ \\end{aligned} \\tag{2.6} \\end{equation} \\] In other words, the size of the error is typically on the order of \\(\\frac{\\sigma}{\\sqrt{n}}\\): the error decreases at a rate of \\(\\sqrt{n}\\). Note that for the expected value of the mean absolute error we have \\(\\mathbb{E} |m_n-m| \\le \\sqrt{\\mathbb{E} \\left[ m_n-m \\right]^2} =\\frac{\\sigma}{\\sqrt{n}}\\) by the Schwartz inequality. What about \\(P(|x-y|&gt;\\varepsilon)\\), the third measure of expected loss we defined in (2.3)? 2.2 Simple non-asymptotic concentration inequalities 2.2.1 Markov’s inequality 2.2.2 Chebychev’s inequality Applying Chebychev to the empirical mean we can show: By ?? we have that with probability \\(1-\\delta\\): \\(|m_n-m|\\le \\frac{\\sigma}{\\sqrt{n\\delta}}\\). This can be shown as follows: if we want to guarantee that \\[ \\begin{aligned} &amp;&amp; P(|m_n-m|&gt;\\varepsilon)&amp;\\le \\delta\\\\ \\end{aligned} \\] then by ?? it suffices that: \\[ \\begin{aligned} &amp;&amp; \\frac{\\sigma^2}{n\\varepsilon^2}&amp;\\le \\delta \\\\ \\end{aligned} \\] This holds if: \\[ \\begin{aligned} &amp;&amp; \\frac{\\sigma}{\\sqrt{n\\delta}}&amp;\\le \\varepsilon\\\\ \\end{aligned} \\] 2.3 Asympotic concentration inequalities 2.3.1 Central Limit Theorem By the Central Limit Theorem we have that \\[ \\begin{aligned} &amp;&amp; P \\left( \\sqrt{n} \\left( \\frac{1}{n} \\sum_{i=1}^{n} x_i-m\\right) \\ge t \\right)&amp; \\rightarrow P(z \\ge t) \\le 2 \\exp(- \\frac{t^2}{2\\sigma^2}) \\\\ \\end{aligned} \\] where \\(z \\sim \\mathcal{N}(0,\\sigma^2)\\). Letting \\(\\varepsilon = \\frac{t}{\\sqrt{n}}\\) we have that asymptotically \\[ \\begin{aligned} &amp;&amp; P \\left( \\sqrt{n} \\left( \\frac{1}{n} \\sum_{i=1}^{n} x_i-m\\right) \\ge \\varepsilon \\right)&amp; \\le 2 \\exp(- \\frac{\\varepsilon^2n}{2\\sigma^2}) \\\\ \\end{aligned} \\] which is a much tigher bound than for example Chebychev. 2.4 Exponential non-asymptotic concentration inequalities The motivation for deriving tight non-asymptotic concentration inequalities is that often we have \\(d &gt;&gt; n\\). In those case we want to control the worst error across all estimators (here the \\(1,...,d\\) empirical means). Hence our goal is to minimize \\[ \\begin{aligned} &amp;&amp; P( \\max_j|m_{n,j}-m|\\ge \\varepsilon) \\\\ \\end{aligned} \\] By the union bound we have \\[ \\begin{aligned} &amp;&amp; P( \\max_j|m_{n,j}-m|\\ge \\varepsilon)&amp;\\le \\sum_{j=1}^{d} P( |m_{n,j}-m|\\ge \\varepsilon)\\\\ \\end{aligned} \\] and by Chebychev (??): \\[ \\begin{aligned} &amp;&amp; \\sum_{j=1}^{d} P( |m_{n,j}-m|\\ge \\varepsilon)&amp;\\le d \\frac{\\sigma^2}{n\\varepsilon^2}\\\\ \\end{aligned} \\] Clearly, if \\(d&gt;&gt;n\\) we are in trouble. Enter: Chernoff bounds. 2.4.1 Chernoff bounds 2.4.2 Hoeffding’s Inequality Using Hoeffding’s Lemma (??) and Chernoff’s bounds (??) we get Hoeffding’s inequality: 2.4.3 Bernstein’s inequality 2.5 Examples Finally, let us at a few examples. Firstly, let us look at what bounds we get for the different concentration inequalities when applied to the binomial distribution: Another illustrative application of Chernoff’s bounds emerges in the context of high-dimensional spaces. Consider the question of how many points we can choose on the \\(d\\)-dimensional unit ball, such that their pair-wise angles are (almost) orthogonal. Hence, if \\(d \\ge \\frac{5.5451774}{\\varepsilon^2}\\) we can choose \\(n= e^{ \\frac{d \\varepsilon^2}{8}}\\) points such that their pair-wise almost orthogonal. 2.6 Appendix 2.6.1 Example of a moment generating function "],
["det-opt.html", "Chapter 3 Optimization 3.1 Line search", " Chapter 3 Optimization 3.1 Line search 3.1.1 Methodology The goal of Exercise 3.1 in Nocedal and Wright (2006) is to minimize the bivariate Rosenbrock function (Equation (3.1)) using steepest descent and Newton’s method. The Rosenbrock function - also known as Rosenbrock’s banana function - has a long, narrow, parabolic shaped flat valley and is often used for to test optimization algorithms for their performance (see here). \\[ \\begin{equation} f(\\mathbb{x})=100(x_2-x_1^2)^2+(1-x_1)^2 \\tag{3.1} \\end{equation} \\] We can implement Equation (3.1) in R as follows: # Rosenbrock: f = function(X) { 100 * (X[2] - X[1]^2)^2 + (1 - X[1])^2 } Figure 3.1 shows the output of the function over \\(x_1,x_2 \\in [-1.5, 1.5]\\) along with its minimum indicated as a red asterisk and the two starting points: (1) \\(X_0=(1.2,1.2)\\) and (2) \\(X_0=(-1.2,1)\\). Figure 3.1: Output of the Rosenbrock function and minimizer in red. The gradient and Hessian of \\(f\\) can be computed as \\[ \\begin{equation} \\nabla f(\\mathbb{x})= \\begin{pmatrix} \\frac{ \\partial f}{\\partial x_1} \\\\ \\frac{ \\partial f}{\\partial x_2} \\end{pmatrix} \\tag{3.2} \\end{equation} \\] and \\[ \\begin{equation} \\nabla^2 f(\\mathbb{x})= \\begin{pmatrix} \\frac{ \\partial^2 f}{\\partial x_1^2} &amp; \\frac{ \\partial^2 f}{\\partial x_1 \\partial x_2} \\\\ \\frac{ \\partial^2 f}{\\partial x_2\\partial x_1} &amp; \\frac{ \\partial^2 f}{\\partial x_2^2} \\end{pmatrix} \\tag{3.3} \\end{equation} \\] which in R can be encoded as follows: # Gradient: df = function(X) { df = rep(0, length(X)) df[1] = -400 * X[1]^2 * (X[2] - X[1]^2) - 2 * (1 - X[1]) # partial with respect to x_1 df[2] = 200 * (X[2] - X[1]^2) return(df) } # Hessian: ddf = function(X) { ddf = matrix(nrow = length(X), ncol=length(X)) ddf[1,1] = 1200 * X[1]^2 - 400 * X[2] + 2 # partial with respect to x_1 ddf[2,1] = ddf[1,2] = -400 * X[1] ddf[2,2] = 200 return(ddf) } For both methods I will use the Arminjo condition with backtracking. The gradient_desc function (below) can implement both steepest descent and Newton’s method. The code for the function can be inspected below. There’s also a small description of the different arguments. function (f, df, X0, step_size0 = 1, ddf = NULL, method = &quot;newton&quot;, c = 1e-04, remember = TRUE, tau = 1e-05, backtrack_cond = &quot;arminjo&quot;, max_iter = 10000) { X_latest = matrix(X0) if (remember) { X = matrix(X0, ncol = length(X0)) steps = matrix(ncol = length(X0)) } iter = 0 if (method == &quot;steepest&quot;) { B = function(X) { diag(length(X)) } } else if (method == &quot;newton&quot;) { B = tryCatch(ddf, error = function(e) { stop(&quot;Hessian needs to be supplied for Newton&#39;s method.&quot;) }) } if (backtrack_cond == &quot;arminjo&quot;) { sufficient_decrease = function(alpha) { return(f(X_k + alpha * p_k) &lt;= f(X_k) + c * alpha * t(df_k) %*% p_k) } } else if (is.na(backtrack_cond)) { sufficient_decrease = function(alpha) { return(f(X_k + alpha * p_k) &lt;= f(X_k)) } } while (any(abs(df(X_latest) - rep(0, length(X_latest))) &gt; tau) &amp; iter &lt; max_iter) { X_k = X_latest alpha = step_size0 df_k = matrix(df(X_latest)) B_k = B(X_latest) p_k = qr.solve(-B_k, df_k) while (!sufficient_decrease(alpha)) { alpha = alpha/2 } X_latest = X_latest + alpha * p_k iter = iter + 1 if (remember) { X = rbind(X, t(X_latest)) steps = rbind(steps, t(alpha * p_k)) } } if (iter &gt;= max_iter) warning(&quot;Reached maximum number of iterations without convergence.&quot;) output = list(optimal = X_latest, visited = tryCatch(X, error = function(e) NULL), steps = tryCatch(steps, error = function(e) NULL), X0 = X0, method = method) return(output) } Similarly you can take a look at how the gradient_desc is applied in the underlying problem by unhiding the next code chunk. library(fromScratchR) init_guesses = 1:nrow(X0) algos = c(&quot;steepest&quot;,&quot;newton&quot;) grid = expand.grid(guess=init_guesses,algo=algos) X_star = lapply( 1:nrow(grid), function(i) { gradient_desc( f=f,df=df, X0=X0[grid[i,&quot;guess&quot;],c(x1,x2)], method = grid[i,&quot;algo&quot;] ) } ) # Tidy up X_star_dt = rbindlist( lapply( 1:length(X_star), function(i) { dt = data.table(X_star[[i]]$visited) dt[,method:=X_star[[i]]$method] dt[,(c(&quot;x0_1&quot;,&quot;x0_2&quot;)):=.(X_star[[i]]$X0[1],X_star[[i]]$X0[2])] dt[,iteration:=.I-1] dt[,y:=f(c(V1,V2)),by=.(1:nrow(dt))] } ) ) 3.1.2 Results The below shows how the two algorithms converge to Figure 3.2: Good initial guess. Figure 3.3: Poor initial guess. "],
["regr.html", "Chapter 4 Regression 4.1 Ordinary least-squares 4.2 Weighted least-squares 4.3 Appendix", " Chapter 4 Regression 4.1 Ordinary least-squares Both OLS and WLS are implemented here using QR decomposition. As for OLS this is very easily done in R. Given some feature matrix X and a corresponding outcome variable y we can use qr.solve(X, y) to compute \\(\\hat\\beta\\). 4.2 Weighted least-squares For the weighted least-squares estimator we have: (see appendix for derivation) \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\hat\\beta_m^{WLS}&amp;= \\left( \\mathbf{X}^T \\Phi^{-1} \\mathbf{X} \\right)^{-1} \\mathbf{X}^T\\Phi^{-1}\\mathbf{y}\\\\ \\end{aligned} \\tag{4.1} \\end{equation} \\] In R weighted this can be implemented as follows: function (X, y, weights = NULL) { if (!is.null(weights)) { Phi &lt;- diag(weights) beta &lt;- qr.solve(t(X) %*% Phi %*% X, t(X) %*% Phi %*% y) } else { beta &lt;- qr.solve(X, y) } return(beta) } 4.3 Appendix 4.3.1 Weighted least-squares Weighted least-squares "],
["class.html", "Chapter 5 Classification 5.1 Binary classification 5.2 Nearest Neighbour 5.3 Logisitic regression 5.4 Appendix", " Chapter 5 Classification 5.1 Binary classification Given an observation \\(X\\in \\mathcal{X} \\subseteq \\mathbb{R}^d\\), we want to assign a binary label (e.g \\(y\\in\\{0,1\\}\\)) to it. A classifier is a function that maps from the feature space to the binary label: \\(g: \\mathcal{X} \\mapsto \\{0,1\\}\\). An observation/label pair is modelled as a pair of random variables \\((X,y)\\). The joint distribution of the pair can be described by: \\[ \\begin{aligned} &amp;&amp; \\mu(A)&amp;=P(X\\in A) \\\\ &amp;&amp; \\eta(X)&amp;=P(Y=1|X=x) \\\\ &amp;&amp; 1-\\eta(X)&amp;=P(Y=0|X=x) \\\\ \\end{aligned} \\] We further have \\[ \\begin{aligned} &amp;&amp; q_0&amp;=P(y=0) \\\\ &amp;&amp; q_1&amp;=P(y=1) \\\\ \\end{aligned} \\] and the class-conditional distributions: \\[ \\begin{equation} \\begin{aligned} &amp;&amp; P(X\\in A|y=0)&amp;, &amp;P(X\\in A |y=1) \\\\ \\end{aligned} \\tag{5.1} \\end{equation} \\] The quality of a classifier \\(g\\) is measured by its risk: \\[ \\begin{equation} \\begin{aligned} &amp;&amp; R(g)&amp;=P(g(X) \\ne y) \\\\ \\end{aligned} \\tag{5.2} \\end{equation} \\] More generally we have a loss function \\(\\ell: \\{0,1\\} \\times \\{0,1\\} \\mapsto \\mathbb{R}\\) and denote \\[ \\begin{equation} \\begin{aligned} &amp;&amp; R(g)&amp;= \\mathbb{E}\\ell(g(X),y)\\\\ \\end{aligned} \\tag{5.3} \\end{equation} \\] which is equivalent to ((5.2)) if we define \\(\\ell(g(X),y)= \\mathbb{1}_{g(X) \\ne y}\\). In the binary case we have \\[ \\begin{aligned} &amp;&amp; R(g)&amp;=P(g(X)\\ne y)=P(g(X)=1,y=0)+P(g(X)=0,y=1) \\\\ &amp;&amp; &amp;=P(g(X)=1|y=0)q_0+P(g(X)=0|y=1)q_1 \\\\ \\end{aligned} \\] Above we conditioned on \\(X=x\\) and in the end stated that taking expectations we have \\(R(g)-R(g^*) \\ge 0\\). More generally we denote \\[ \\begin{equation} \\begin{aligned} &amp;&amp; R(g)&amp;= \\mathbb{E} \\left[ \\mathbb{1}_{g(X)=1}(1-\\eta(X)) +\\mathbb{1}_{g(X)=0} \\eta(X) \\right]\\\\ \\end{aligned} \\tag{5.4} \\end{equation} \\] and for the Bayes risk therefore: \\[ \\begin{equation} \\begin{aligned} &amp;&amp; R^*&amp;=\\mathbb{E} \\left[ \\mathbb{1}_{\\eta(X)\\ge \\frac{1}{2}}(1-\\eta(X)) +\\mathbb{1}_{\\eta(X)&lt;0} \\eta(X) \\right] \\\\ &amp;&amp; &amp;= \\mathbb{E} \\min (\\eta(X),1-\\eta(X))\\\\ \\Rightarrow&amp;&amp; R^*&amp;\\in(0, \\frac{1}{2}) \\\\ \\end{aligned} \\tag{5.5} \\end{equation} \\] Of course, in practice \\(\\eta(X)\\) is unknown and instead estimated through supervised learning using training data \\(D_n=((X_1,y_1),...,(X_n,y_n))\\). We denote a data-dependent classifier as \\[ \\begin{equation} \\begin{aligned} &amp;&amp; g_n&amp;=g_n(X,D_n)\\in\\{0,1\\} \\\\ \\end{aligned} \\tag{5.6} \\end{equation} \\] Our goal in classification is to find a classifier such that \\(\\mathbb{E}R(g_n)-R^*\\) is small. 5.2 Nearest Neighbour Assume that \\(\\mathcal{X}\\) is a metric space. Nearest Neighbour rules are based on how far away points are from each other based on some metric distance. Classifiers based on such rules assign labels to points based on the labels their neighbours. 5.2.1 1NN It can be shown that that the distance between any point and its nearest neighbour is typically on the order of \\(n^{-\\frac{1}{d}}\\). Hence for \\(n\\rightarrow \\infty\\) we have that \\(d(X_{(1)}(X),X)\\rightarrow0\\). Consequently, for \\(n\\) sufficiently large \\[ \\begin{aligned} &amp;&amp; X_{(1)}(X)&amp;\\approx X \\\\ &amp;&amp; \\eta(X_{(1)}(X))&amp;\\approx \\eta(X) \\\\ \\end{aligned} \\] Let \\(y \\sim \\text{Bern}(\\eta)\\), then \\[ \\begin{aligned} &amp;&amp; P(y_{(1)}(X)\\ne y)&amp;=P(y_{(1)}(X)=1, y=0)+P(y_{(1)}(X)=0, y=1) \\\\ &amp;&amp; &amp;\\approx \\eta(1-\\eta)+\\eta(1-\\eta)=2\\eta(1-\\eta) \\\\ \\end{aligned} \\] The inequality can be derived as follows. Recall that \\(R^*= \\mathbb{E} \\left[ \\min(\\eta,1-\\eta) \\right]\\) and note that \\(\\eta(1-\\eta)\\le\\min(\\eta(1-\\eta))\\) for \\(\\eta\\in[0,1]\\). Hence, clearly \\(R^*\\le R^{\\text{1NN}}\\le2R^*\\). Let \\(Z=\\min(\\eta,1-\\eta)\\), then since \\(Z(1-Z)\\) is concave we can apply Jensen’s Inequality to derive the final result in ??. 5.2.2 KNN One can show that for the asymptotic risk of the KNN-classifier we have: \\[ \\begin{equation} \\begin{aligned} &amp;&amp; R^{(\\text{KNN})}&amp;=R^*+ \\frac{1}{\\sqrt{ke}} \\\\ \\end{aligned} \\tag{5.7} \\end{equation} \\] 5.3 Logisitic regression To model the probability of \\(y=1\\) we will use logistic regression: \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\mathbf{p}&amp;= \\frac{ \\exp( \\mathbf{X} \\beta )}{1 + \\exp(\\mathbf{X} \\beta)} \\end{aligned} \\tag{5.8} \\end{equation} \\] Equation (5.8) is not estimated directly but rather derived from linear predictions \\[ \\begin{equation} \\begin{aligned} \\log \\left( \\frac{\\mathbf{p}}{1-\\mathbf{p}} \\right)&amp;= \\mathbf{X} \\beta \\\\ \\end{aligned} \\tag{5.9} \\end{equation} \\] where \\(\\beta\\) can be estimated through iterative re-weighted least-squares (IRLS) which is a simple implementation of Newton’s method (see for example Wasserman (2013); a complete derivation can also be found in the appendix): \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\beta_{s+1}&amp;= \\left( \\mathbf{X}^T \\mathbf{W} \\mathbf{X} \\right) \\mathbf{X}^T \\mathbf{W}z\\\\ \\text{where}&amp;&amp; z&amp;= \\mathbf{X}\\beta_{s} + \\mathbf{W}^{-1} (\\mathbf{y}-\\mathbf{p}) \\\\ &amp;&amp; \\mathbf{W}&amp;= \\text{diag}\\{p_i(\\beta_{s})(1-p_i(\\beta_{s}))\\}_{i=1}^n \\\\ \\end{aligned} \\tag{5.10} \\end{equation} \\] In R this can be implemented from scratch as below. For the empirical exercises we will rely on glm([formula], family=\"binomial\") which scales much better to higher dimensional problems than my custom function and also implements weighted logit. function (X, y, beta_0 = NULL, tau = 1e-09, max_iter = 10000) { if (!all(X[, 1] == 1)) { X &lt;- cbind(1, X) } p &lt;- ncol(X) n &lt;- nrow(X) if (is.null(beta_0)) { beta_latest &lt;- matrix(rep(0, p)) } W &lt;- diag(n) can_still_improve &lt;- T iter &lt;- 1 while (can_still_improve &amp; iter &lt; max_iter) { y_hat &lt;- X %*% beta_latest p_y &lt;- exp(y_hat)/(1 + exp(y_hat)) df_latest &lt;- crossprod(X, y - p_y) diag(W) &lt;- p_y * (1 - p_y) Z &lt;- X %*% beta_latest + qr.solve(W) %*% (y - p_y) beta_latest &lt;- qr.solve(crossprod(X, W %*% X), crossprod(X, W %*% Z)) can_still_improve &lt;- mean(abs(df_latest)) &gt; tau iter &lt;- iter + 1 } return(list(fitted = p_y, coeff = beta_latest)) } 5.4 Appendix 5.4.1 Iterative reweighted least-squares Iterative reweighted least-squares "],
["emp.html", "Chapter 6 Empirical risk minimization 6.1 Excess risk and overfitting error 6.2 Rademacher averages", " Chapter 6 Empirical risk minimization 6.1 Excess risk and overfitting error Let \\(g:\\mathcal{X}\\mapsto\\{0,1\\}\\) be a classifier. Given data \\(D_n=((X_1,y_1),...,(X_n,y_n))\\) we can estimate \\(R(g)=P(g(X)\\ne y)\\) by the empirical mean \\(R_n(g)= \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}_{g(X_i)\\ne y_i}\\). Then by Hoeffding’s Inequality we have \\[ \\begin{aligned} &amp;&amp; P(\\left|R_n(g)-R(g)\\right|\\ge \\varepsilon)&amp;\\le 2e^{-2n\\varepsilon^2} \\\\ \\end{aligned} \\] and equivalently with probability \\(\\ge 1-\\delta\\): \\[ \\begin{aligned} &amp;&amp; \\left|R_n(g)-R(g)\\right|&amp;\\le \\sqrt{ \\frac{\\log( \\frac{2}{\\delta})}{2n}} \\\\ \\end{aligned} \\] Suppose now that we have a set \\(N\\) classifiers \\(\\mathcal{C}=\\{g^{(1)},...,g^{(n)}\\}\\). Now let \\(g_n\\) denote the data-based classifier that minimizes the empirical risk \\(R_n(g^{(j)})\\) among our \\(N\\) classifiers. For its risk \\(R(g_n)=P(g_n(X)\\ne y|D_n)\\) we can establish two quantities of interest: We can further establish two basic inequalities. The first one is trivial: \\[ \\begin{equation} \\begin{aligned} &amp;&amp; R(g_n)-R_n(g_n)&amp;\\le \\max_{j=1,...,N} \\left| R(g^{(j)})-R_n(g^{(j)}) \\right| &amp;&amp; \\text{(overfitting)} \\\\ \\end{aligned} \\tag{6.1} \\end{equation} \\] To derive the second one, note that we can rewrite the term in ?? as \\[ \\begin{aligned} &amp;&amp; R(g_n)-\\min_{j=1,..,N}R(g^{(j)})&amp;=R(g_n)-R_n(g_n)+R_n(g_n)-\\min_{j=1,..,N}R(g^{(j)}) \\\\ \\end{aligned} \\] where \\(R(g_n)-R_n(g_n)\\) just correspond to the overfitting error again. For the second term denote \\(\\bar{g}=\\arg\\min_{j}R(g^{(j)})\\) and note that \\[ \\begin{aligned} &amp;&amp; R_n(g_n)-\\min_{j=1,..,N}R(g^{(j)})&amp;\\le R_n(\\bar{g})-\\min_{j=1,..,N}R(g^{(j)}) \\\\ \\end{aligned} \\] since by definition \\(g_n\\) minimizes the empirical risk and hence \\(R_n(g_n)\\le R_n(\\bar{g})\\). Once again we can trivially establish that \\[ \\begin{aligned} &amp;&amp; R_n(\\bar{g})-\\min_{j=1,..,N}R(g^{(j)})&amp;\\le \\max_{j=1,...,N} \\left| R(g^{(j)})-R_n(g^{(j)})\\right|\\\\ \\end{aligned} \\] which is the just the bound for the overfitting error already established in (6.1). Hence, we take everything together to arrive at the second basic inequality: \\[ \\begin{equation} \\begin{aligned} &amp;&amp; R(g_n)-\\min_{j=1,..,N}R(g^{(j)})&amp;\\le 2\\max_{j=1,...,N} \\left| R(g^{(j)})-R_n(g^{(j)})\\right| &amp;&amp; \\text{(excess risk)} \\\\ \\end{aligned} \\tag{6.2} \\end{equation} \\] So both the excess risk and the overfitting error may be bounded in term of: \\[ \\begin{aligned} &amp;&amp; \\max_{j=1,...,N} \\left| R(g^{(j)})-R_n(g^{(j)})\\right| \\\\ \\end{aligned} \\] Now let us actually derive a bound. We have \\[ \\begin{equation} \\begin{aligned} &amp;&amp; P\\left(\\max_{j=1,...,N} \\left| R(g^{(j)})-R_n(g^{(j)})\\right|\\ge\\varepsilon\\right)&amp;=P\\left(\\bigcup_{j=1,...,N} \\left\\{ \\left| R(g^{(j)})-R_n(g^{(j)})\\right|\\ge\\varepsilon\\right\\}\\right) \\\\ &amp;&amp; &amp;\\le \\sum_{j=1}^{N}P\\left(\\left| R(g^{(j)})-R_n(g^{(j)})\\right|\\ge\\varepsilon\\right) \\\\ &amp;&amp; &amp;\\le 2Ne^{-2n\\varepsilon^2} \\\\ \\end{aligned} \\tag{6.3} \\end{equation} \\] where the first inequality follows from the union bound and the second one from Hoeffding’s Inequality. Equivalently, we finally have that with probability \\(\\ge 1-\\delta\\) \\[ \\begin{aligned} &amp;&amp; \\max_{j=1,...,N} \\left| R(g^{(j)})-R_n(g^{(j)})\\right|&amp;\\le \\sqrt{ \\frac{\\log ( \\frac{2N}{\\delta})}{2n}} \\\\ \\end{aligned} \\] and hence the following bound: \\[ \\begin{equation} \\begin{aligned} &amp;&amp; R(g_n)-R_n(g_n)&amp;\\le\\sqrt{ \\frac{\\log ( \\frac{2N}{\\delta})}{2n}} &amp;&amp; \\text{(overfitting)} \\\\ \\end{aligned} \\tag{6.4} \\end{equation} \\] \\[ \\begin{equation} \\begin{aligned} &amp;&amp; R(g_n)-\\min_{j=1,..,N}R(g^{(j)})&amp;\\le 2\\sqrt{ \\frac{\\log ( \\frac{2N}{\\delta})}{2n}} &amp;&amp; \\text{(excess risk)} \\\\ \\end{aligned} \\tag{6.5} \\end{equation} \\] 6.1.1 Data splitting Let \\(\\mathcal{C}=\\{g_1^{(1)},...,g_n^{(N)}\\}\\) be a set of data-dependent classifiers depending on \\(D_n\\) and suppose that an independent data set is available for testing \\(D&#39;_m=((X&#39;_1,y&#39;_1),...,(X&#39;_m,y&#39;_m))\\). Then we may estimate the true risk \\(R(g_n^{(j)})=P(g_n^{(j)}(X)\\ne y|D_n)\\) by the empirical risk (test error): \\[ \\begin{equation} \\begin{aligned} &amp;&amp; R&#39;_m(g_n^{(j)})&amp;= \\frac{1}{m} \\sum_{i=1}^{m} \\mathbb{1}_{g_n^{(j)}(X&#39;_i)\\ne y&#39;_i}\\\\ \\end{aligned} \\tag{6.6} \\end{equation} \\] Then using the results from above, where have for the empirical risk minimizer \\(g_{n,m}=\\arg\\min_{j=1,...,N}R&#39;_m(g_n^{(j)})\\) that with probability \\(1-\\delta\\): \\[ \\begin{aligned} &amp;&amp; R(g_{n,m})-R&#39;_m(g_{n,m})&amp;=\\sqrt{ \\frac{\\log ( \\frac{2N}{\\delta})}{2m}} &amp;&amp; \\text{(overfitting)} \\\\ \\end{aligned} \\] \\[ \\begin{aligned} &amp;&amp; R(g_{n,m})-\\min_{j=1,..,N}R(g_n^{(j)})&amp;\\le 2\\sqrt{ \\frac{\\log ( \\frac{2N}{\\delta})}{2m}} &amp;&amp; \\text{(excess risk)} \\\\ \\end{aligned} \\] 6.1.2 Leave-one-out cross-validation Instead of data-splitting (once) we can use leave-one-out cross-validation to get an estimate of the true risk of our data-based classifier. As before we have \\(D_n=((X_1,y_1),...,(X_n,y_n))\\). Now let \\(D_{n,i}=((X_1,y_1),...,(X_{i-1},y_{i-1}),(X_{i+1},y_{i+1}),...,(X_n,y_n))\\) denote the subsample that contains all observations except \\(i\\). Then we can define: \\[ \\begin{aligned} &amp;&amp; R_n^{(D)}(g_n)&amp;= \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}_{g_{n-1}(X_i,D_{n,i})\\ne y_i} \\\\ \\end{aligned} \\] Now since \\(g_{n-1}\\) does not depend on \\((X_i,y_i)\\) we have that: \\[ \\begin{aligned} &amp;&amp; \\mathbb{E} \\left[ R_n^{(D)}(g_n) \\right]&amp;= \\mathbb{E} \\left[ R(g_{n-1}) \\right]\\\\ \\end{aligned} \\] But since \\(\\mathbb{E} \\left[ R(g_{n-1}) \\right]\\ne\\mathbb{E} \\left[ R(g_{n}) \\right]\\) we introduce a small amount of bias. In general it is not easy to establish bounds for the leave-one-out estimator, but empirically it performs well. 6.1.3 Realizable case Let \\(\\mathcal{C}=\\{g^{(1)},...,g^{(N)}\\}\\) be non-data-dependent classifiers depending on \\(D_n\\). Now assume one of the candidate classifier has zero risk, that is \\(\\min_jR(g^{(j)})=0\\). We refer to this as the realizable case. Note that this implies that \\(\\min_jR_n(g^{(j)})=0\\) and also that both the excess risk and overfitting error are equal to the true risk, \\(R(g_n)\\). Hence, we would like to bound this quantity. \\[ \\begin{aligned} &amp;&amp; P(R(g_n) \\ge \\varepsilon)&amp;\\le P \\left( \\exists j\\in 1,...,N: R(g^{(j)})\\ge \\varepsilon; R_n(g^{(j)})=0 \\right)\\\\ &amp;&amp; &amp;\\le N P \\left(R(g)\\ge \\varepsilon \\ \\&amp; \\ R_n(g)=0 \\right) \\\\ &amp;&amp; &amp;\\le N(1-\\varepsilon)^N \\le N e^{-n\\varepsilon} \\\\ \\end{aligned} \\] where the second inequality follows from the union bound. 6.2 Rademacher averages In many interesting cases the class of classifiers \\(\\mathcal{C}\\) that we wish to consider contains infinitely many classifiers. A common example is the class of deep neural networks, that can be arbitrarily deep and wide. To control overfitting in such cases we try to bound: \\[ \\begin{aligned} &amp;&amp; \\max_{g\\in\\mathcal{C}}\\left|R(g)-R_n(g)\\right| \\\\ \\end{aligned} \\] At this point we shall simplify notation a little bit. As before, let \\(X_1,...,X_n\\) be iid \\(\\in \\mathcal{X}\\). For a set \\(A\\subset\\mathcal{X}\\) we denote \\[ \\begin{aligned} &amp;&amp; P(A)&amp;=P(X\\in A) &amp;&amp; \\text{(true probability)}\\\\ &amp;&amp; P_n(A)&amp;= \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}_{X_i\\in A} &amp;&amp; \\text{(empirical frequency)}\\\\ \\end{aligned} \\] where \\(X\\in A\\) in the context of empirical risk minimization corresponds to the classifier committing error. Now let \\(\\mathcal{A}\\) be a class of subsets of \\(\\mathcal{X}\\). Our aim here is to understand \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\max_{A\\in\\mathcal{A}} |P_n(A)-P(A)| \\\\ \\end{aligned} \\tag{6.7} \\end{equation} \\] which looks similar to the expression for overfitting defined earlier ??. 6.2.1 Finite class of classifiers Suppose first that \\(\\mathcal{A}\\) is a finite class. Then in order to bound (6.7) we can proceed in the same way as we did before for the overfitting error and excess risk (Equation (6.3)), where we used Hoeffding’s Inequality and the union bound. Here we have that with probability \\(\\ge 1-\\delta\\): \\[ \\begin{aligned} &amp;&amp; \\max_{A\\in\\mathcal{A}} |P_n(A)-P(A)|&amp;\\le \\sqrt{ \\frac{\\log ( \\frac{2N}{\\delta})}{2n}} \\\\ \\end{aligned} \\] This bound applies for given \\(A\\). Now let us go a step further and bound the expected value of (6.7). To do so we use a little trick where we first take the exponential, in particular \\[ \\begin{aligned} &amp;&amp; \\exp\\left\\{\\lambda \\mathbb{E} \\left[ \\max_{A\\in\\mathcal{A}} (P_n(A)-P(A)) \\right]\\right\\}&amp;\\le \\mathbb{E} \\left[ \\exp \\left\\{ \\lambda\\max_{A\\in\\mathcal{A}} (P_n(A)-P(A)) \\right\\} \\right] &amp;&amp;, &amp; \\lambda&gt;0\\\\ &amp;&amp; &amp;=\\mathbb{E} \\left[ \\max_{A\\in\\mathcal{A}} \\left( \\exp \\left\\{ \\lambda (P_n(A)-P(A)) \\right\\}\\right) \\right] \\\\ &amp;&amp; &amp;\\le \\mathbb{E} \\left[ \\sum_{A\\in\\mathcal{A}}\\exp \\left\\{ \\lambda (P_n(A)-P(A)) \\right\\}\\right]\\\\ &amp;&amp; &amp;= \\sum_{A\\in\\mathcal{A}}\\mathbb{E} \\left[ \\exp \\left\\{ \\lambda (P_n(A)-P(A)) \\right\\}\\right] \\\\ \\end{aligned} \\] where the the first step is by Jensen’s Inequality, the second step holds since \\(e^{\\lambda x}\\) is strictly increasing, the inequality in the third step is trivial and the final step is by linearity of expectations. Now note that \\(\\mathbb{E} \\left[ \\exp \\left\\{ \\lambda (P_n(A)-P(A)) \\right\\}\\right]\\) is just the moment generating function of the binomial distribution. As we saw in Chapter 2.4.2, we can use Hoeffding’s Lemma to bound that quantity, in particular: \\[ \\begin{aligned} &amp;&amp; \\exp\\left\\{\\lambda \\mathbb{E} \\left[ \\max_{A\\in\\mathcal{A}} (P_n(A)-P(A)) \\right]\\right\\}&amp;\\le \\sum_{A\\in\\mathcal{A}}\\mathbb{E} \\left[ \\exp \\left\\{ \\lambda (P_n(A)-P(A)) \\right\\}\\right]\\\\ &amp;&amp; &amp;\\le \\sum_{A\\in\\mathcal{A}} e^{ \\frac{\\lambda^2}{8n}}=Ne^{ \\frac{\\lambda^2}{8n}} \\\\ \\end{aligned} \\] Finally, taking logarithms and dividing by \\(\\lambda\\), we get: \\[ \\begin{aligned} &amp;&amp; \\mathbb{E} \\left[ \\max_{A\\in\\mathcal{A}} (P_n(A)-P(A)) \\right]&amp;\\le \\frac{\\log N}{\\lambda}+ \\frac{\\lambda}{8n} \\\\ \\end{aligned} \\] Taking the first-order condition with respect to \\(\\lambda\\) we get \\(\\lambda^*=\\sqrt{8n\\log N}\\) and hence we can finally establish: \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\mathbb{E} \\max_{A\\in\\mathcal{A}} |P_n(A)-P(A)| &amp;\\le 2 \\sqrt{ \\frac{\\log N}{2n}}=\\sqrt{ \\frac{2\\log N}{n}} \\\\ \\end{aligned} \\tag{6.8} \\end{equation} \\] 6.2.2 Infinitely many classifiers As pointed out above, there are many interesting case in which the class \\(\\mathcal{A}\\) counts infinitely many classifiers. To establish a bound for such cases, we will use two symmetrization tricks: the first one involves the introduction of a ‘ghost’ sample; the second one involves so called Rademacher random variables. Starting with the former, let \\(X&#39;_1,...,X&#39;_n\\) be our ‘ghost’ sample of iid data following the same distribution as \\(X\\). "],
["regularization.html", "Chapter 7 Regularization 7.1 Bias-variance tradeoff", " Chapter 7 Regularization 7.1 Bias-variance tradeoff To set the stage for the remainder of this note we will briefly revisit the bias-variance trade-off in this section. In particular we will illustrate the effect of varying the sample size \\(n\\). Readers familiar with this topic may choose to skip this section. As as in Bishop (2006) we consider synthetic data generated by the sinusoidal function \\(f(x)=\\sin(2\\pi x)\\). To simulate random samples of \\(\\mathbf{y}\\) we sample \\(n\\) input values from \\(\\mathbf{X} \\sim \\text{unif}(0,1)\\) and introduce a random noise component \\(\\varepsilon \\sim \\mathcal{N}(0,0.3)\\). Figure 7.1 shows \\(\\mathbf{y}\\) along with random draws \\(\\mathbf{y}^*_n\\). Figure 7.1: Sinusoidal function and random draws. Following Bishop (2006) we will use a Gaussian linear model with Gaussian kernels \\(\\exp(-\\frac{(x_k-\\mu_p)^{2}}{2s^2})\\) as \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\mathbf{y}|\\mathbf{X}&amp; =f(x) \\sim \\mathcal{N} \\left( \\sum_{j=0}^{p-1} \\phi_j(x)\\beta_j, v \\mathbb{I}_p \\right) \\\\ \\end{aligned} \\tag{7.1} \\end{equation} \\] with \\(v=0.3\\) to estimate \\(\\hat{\\mathbf{y}}_k\\) from random draws \\(\\mathbf{X}_k\\). We fix the number of kernels \\(p=24\\) (and hence the number of features \\(M=p+1=25\\)) as well as the spatial scale \\(s=0.1\\). To vary the complexity of the model we use a form of regularized least-squares (Ridge regression) and let the regularization parameter \\(\\lambda\\) vary \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\hat\\beta&amp;=(\\lambda I + \\Phi^T \\Phi)^{-1}\\Phi^Ty \\\\ \\end{aligned} \\tag{7.2} \\end{equation} \\] where high values of \\(\\lambda\\) in (7.2) shrink parameter values towards zero. (Note that a choice \\(\\lambda=0\\) corresponds to the OLS estimator which is defined as long as \\(p \\le n\\).) As in Bishop (2006) we proceed as follows for each choice of \\(\\lambda\\) and each sample draw to illustrate the bias-variance trade-off: Draw \\(N=25\\) time from \\(\\mathbf{u}_k \\sim \\text{unif}(0,1)\\). Let \\(\\mathbf{X}_k^*=\\mathbf{u}_k+\\varepsilon_k\\) with \\(\\varepsilon \\sim \\mathcal{N}(0, 0.3)\\). Compute \\(\\mathbf{y}_k^*=\\sin(2\\pi \\mathbf{X}^*_k)\\). Extract features \\(\\Phi_k\\) from \\(\\mathbf{X}_k^*\\) and estimate the parameter vector \\(\\beta_k^*(\\Phi_k,\\mathbf{y}^*_k,\\lambda)\\) through regularized least-squares. Predict \\(\\hat{\\mathbf{y}}_k^*=\\Phi \\beta_k^*\\). Applying the above procedure we can construct the familiar picture that demonstrates how increased model complexity increases variance while reducing bias (Figure 7.2). Recall that for the mean-squared error (MSE) we have \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\mathbb{E} \\left( (\\hat{f}_n(x)-f(x))^2 \\right) &amp;= \\text{var} (\\hat{f}_n(x)) + \\left( \\mathbb{E} \\left( \\hat{f}_n(x) \\right) - f(x) \\right)^2 \\\\ \\end{aligned} \\tag{2.1} \\end{equation} \\] where the first term on the right-hand side corresponds to the variance of our prediction and the second term to its (squared) bias. In Figure 7.2 as model complexity increases the variance component of the MSE increases, while the bias term diminishes. A similar pattern would have been observed if instead of using regularization we had used OLS and let the number of Gaussian kernels (and hence the number of features \\(p\\)) vary where higher values of \\(p\\) correspond to increased model complexity. Figure 7.2: Bias-variance trade-off "],
["dim-red.html", "Chapter 8 Dimensionality reduction 8.1 Random projections 8.2 PCA 8.3 PCA for feature extraction 8.4 High-dimensional data 8.5 Forward search", " Chapter 8 Dimensionality reduction Let \\(X_1,...,X_n\\in\\mathbb{R}^p\\) be our high-dimensional data. We want to project the data onto a lower-dimensional space, that is we want \\(q&lt;&lt;p\\) and \\(f:\\mathbb{R}^p \\mapsto \\mathbb{R}^q\\) such that \\(||f(X_k)-f(X_l)|| \\approx ||X_k-X_l||\\) – the distance between any two points in the lower-dimensional space should be close to their corresponding distance in the original space. We can respecify the problem as \\[ \\begin{aligned} &amp;&amp; \\max_{k,l\\le n}\\left| \\frac{||f(X_k)-f(X_l)||^2}{||X_k-X_l||^2}-1\\right|&amp;\\le \\varepsilon\\\\ \\end{aligned} \\] where \\(\\varepsilon\\) is some desired accuracy. 8.1 Random projections Let \\(f(X)=WX\\), \\(X\\in\\mathbb{R}^p\\), \\(W\\) \\((q\\times p)\\) and for the elements of \\(W\\): \\(w_{ij} \\sim \\mathcal{N}(0, \\frac{1}{q})\\) are idd. First note that \\[ \\begin{aligned} &amp;&amp; \\mathbb{E}||f(X)||^2&amp;= \\mathbb{E} ||W X||^2= \\mathbb{E} \\sum_{i=1}^{q} \\left( \\sum_{j=1}^{p} X_jW_{i,j}\\right)^2\\\\ &amp;&amp; &amp;= \\sum_{i=1}^{q}\\mathbb{E} \\left( \\sum_{j=1}^{p} X_jW_{i,j}\\right)^2= \\sum_{i=1}^{q} \\frac{||X||^2}{q}\\\\ \\end{aligned} \\] where the last equality follows from \\(\\sum_{j=1}^{p} X_jW_{i,j} \\sim \\mathcal{N}(0, \\frac{1}{q}\\sum_{j=1}^{p} X_j^2 ) = \\sim \\mathcal{N}(0,\\frac{||X||^2}{q})\\). Then finally: \\[ \\begin{aligned} &amp;&amp; \\mathbb{E}||f(X)||^2&amp;=||X||^2 \\\\ \\end{aligned} \\] Now in particular for any \\(i,j\\le n\\) \\[ \\begin{aligned} &amp;&amp; \\mathbb{E}||f(X_k)-f(X_l)||^2&amp;= \\mathbb{E}||W(X_k-X_l)||^2=||X_k-X_l||^2 \\\\ \\end{aligned} \\] Now for the actual proof: This is quite amazing since the final result is independent of \\(p\\). Let \\(p=10^{6}\\) and \\(n=100\\). Then with probability \\(0.9\\) we have \\[ \\begin{aligned} &amp;&amp; \\max_{k,l\\le n}\\left| \\frac{||f(X_k)-f(X_l)||^2}{||X_k-X_l||^2}-1\\right| &amp;\\le 0.01 \\\\ \\end{aligned} \\] whenever \\(q\\ge2.48585\\times 10^{5}\\). 8.2 PCA Another common way to reduce model dimensionality is through principal component analysis (PCA). Very loosely defined principal components can be thought of as describing the main sources of variation in the data. While in theory any design matrix \\(X\\) \\((n \\times p)\\) can be decomposed into its principal components, in practice PCA can be more or less useful for dimensionality reduction depending on how the \\(p\\) different features in \\(X\\) related to each other. We will see that in particular for highly correlated data PCA can be an extremely useful tool for dimensionality reduction. 8.2.1 The maths behind PCA PCA projects \\(X\\) into a \\(q\\)-dimensional space where \\(q \\le p\\), such that the covariance matrix of the \\(q\\)-dimensional projection is maximised. Intuitively, we want to find the linear combination of points in \\(X\\) which explains the largest part of the variance in \\(X\\). Formally (in a very stylised fashion) this amounts to \\[ \\begin{aligned} &amp;&amp; \\max_a&amp; \\left( \\Omega = P_1^TP_1 = v^TX^TXv = v^T \\Sigma v \\right) \\\\ \\text{s.t.} &amp;&amp; v^Tv&amp;= \\mathbf{1} &amp;&amp; \\text{(loading vector)} \\\\ \\text{where}&amp;&amp; P_1 &amp;= v^T X &amp;&amp; \\text{(principal component)} \\\\ \\end{aligned} \\] where \\(v\\) is given by the eigenvector corresponding to the largest eigenvalue of \\(\\Sigma\\) - the covariance matrix of \\(X\\). We can eigen-decompose \\(\\Sigma\\) \\[ \\begin{aligned} &amp;&amp; \\Sigma&amp;= V \\Lambda V^{-1} \\\\ \\end{aligned} \\] where \\(\\Lambda\\) is a diagonal matrix of eigenvalues in decreasing order. Sometimes eigenvectors \\(V\\) are referred to as rotation vectors: the first eigenvector - i.e. the one corresponding to the highest eigenvalue - rotates the data into the direction of the highest variation. Remembers this image from the brush-ups? Projecting into orthogonal subspace 8.2.2 An intuitive example PCA can be applied very well to highly correlated time series. Take for example US treasury yields of varying maturities over time: they are intrinsically linked to each other through the term-structure of interest rates. As we will see the first couple of principal components have a very intuitive interpretation when applied to yield curves. Let’s perform PCA through spectral decomposition in R and look at the output. percent 98.0788860 1.6079986 0.3004019 0.0064192 0.0027590 0.0014110 Let’s compute the first three principal components and plot them over time. How can we make sense of this? It is not obvious and in many applications interpreting the components at face value is a difficult task. In this particular example it turns out that we can actually make sense of them. Let’s see what happens when we play with the components. (Shiny app not run in static HTML) 8.3 PCA for feature extraction 8.3.1 Squared elements of eigenvectors Consider again using the spectral decomposition of \\(\\Sigma=\\mathbf{X}^T\\mathbf{X}\\) to perform PCA: \\[ \\begin{aligned} &amp;&amp; \\Sigma&amp;= V \\Lambda V^{T} \\\\ \\end{aligned} \\] A practical complication with PCA is that generally the principal components cannot be easily interpreted. If we are only interested in prediction, then this may not be a concern. But when doing inference, we are interested in the effect of specific features rather than the principal components that describe the variation in the design matrix \\(\\mathbf{X}\\). It turns out that we can still use spectral decomposition to select features directly. The clue is to realize that the \\(i\\)-th squared element \\(v^2_{ji}\\) of the \\(j\\)-th eigenvector can be though of as the percentage contribution of feature \\(i\\) to the variation in the \\(j\\)-th principal component. Having already established above that eigenvalues provide a measure of how much of the overall variation in \\(X\\) is explained by the \\(j\\)-th principal component, these two ideas can be combined to give us a straight-forward way to identify important features (for explaining variation in \\(\\mathbf{X}\\)). In particular, compute \\(\\mathbf{s}= \\text{diag}\\{\\Lambda\\}/ \\text{tr} (\\Lambda)\\) \\((p \\times 1)\\) where \\(\\Lambda\\) is diagonal matrix of eigenvalues and let \\(\\mathbf{V}^2\\) be the matrix of eigenvectors with each elements squared. Consider computing the following vector \\(\\mathbf{r} \\in \\mathbb{R}^p\\) which will use to rank our \\(p\\) features: \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\mathbf{r}&amp;=\\mathbf{V}^2 \\mathbf{s} \\\\ \\end{aligned} \\tag{8.1} \\end{equation} \\] By construction elements in \\(\\mathbf{r}\\) sum up to one so they can be thought of as percentages describing the overall importance of individual features in terms of explaining the overall variation in the design matrix. This last point is important: PCA never even looks at the outcome variable that we are interested in modelling. Even features identified as not important by PCA may in fact be very important for the model, so the ranking is merely a guideline of sorts. Then say we were interested in decreasing the number of features from \\(p=12\\) to \\(q=5\\), then this approach suggests using the following features: feature importance 30 YR 0.1328828 20 YR 0.1146219 10 YR 0.0887639 7 YR 0.0803942 6 MO 0.0751135 8.3.2 SVD Instead of using spectral decomposition we could use (compact) SVD for PCA. Compact SVD works decomposes any matrix \\(X\\) \\((n \\times p)\\) as follows \\[ \\begin{aligned} &amp;&amp; X&amp;= U S V^{T} \\\\ \\end{aligned} \\] where \\(S\\) is diagonal \\((p\\times p)\\), \\(U\\) is \\((n \\times r)\\) and \\(V\\) is \\((r \\times p)\\) with \\(r=\\min(n,p)\\). The diagonal elements of \\(S\\) are referred to as singular values (Note: it turns out that they correspond to the square roots of the eigenvalues of \\(X^TX\\)). It is then easy to see that \\(V\\) is once again the matrix of eigenvectors of \\(X^tX\\) as above \\[ \\begin{aligned} &amp;&amp; \\Sigma&amp;=X^TX=(V^TS^TU^T)US V \\\\ &amp;&amp; &amp;= V(S^TS)V^T=V\\Lambda V^T\\\\ \\end{aligned} \\] where \\(U^TU=I\\) by the fact that \\(U\\) is orthogonal and from the note above it should be clear why \\(S^TS=\\Lambda\\). Unsurprisingly this gives us the equivalent ranking of features: feature importance 30 YR 0.1491249 20 YR 0.1188201 10 YR 0.0816704 1 MO 0.0778362 2 MO 0.0750598 8.4 High-dimensional data Now that we have developed a good intuition of PCA, it is time to face its shortfalls. While PCA can be used as tool to reduce dimensionality it is actually inconsistent for cases where \\(p&gt;&gt;n\\). Enter: regularized SVD. 8.4.1 Regularized SVD Witten, Tibshirani, and Hastie (2009) propose a regularized version of SVD where the \\(L_1\\)-norm eigenvectors \\(\\mathbf{v}_k\\) is penalized. In particular the authors propose a modified version of the following optimization problem: \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\max_a&amp; \\left( \\Omega = P_1^TP_1 = \\mathbf{v}^TX^TX\\mathbf{v} = \\mathbf{v}^T \\Sigma \\mathbf{v} \\right) \\\\ \\text{s.t.} &amp;&amp; \\mathbf{v}^T\\mathbf{v}&amp;= \\mathbf{1} \\\\ &amp;&amp; |\\mathbf{v}|&amp; \\le c \\\\ \\end{aligned} \\tag{8.2} \\end{equation} \\] Note that this essentially looks like spectral decomposition with an added LASSO penality: consequently depending on the regularization parameter \\(c\\) some elements of \\(\\mathbf{v}_k\\) will be shrinked to exactly zero and hence this form of penalized SVD is said yield sparse principal components. The optimization problem in (8.2) - originally proposed Jolliffe, Trendafilov, and Uddin (2003) - is non-convex and hence computationally hard. Witten, Tibshirani, and Hastie (2009) propose to instead solve the following optimization problem: \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\max_a&amp; \\left( \\mathbf{u}^TX\\mathbf{v} \\right)\\\\ \\text{s.t.} &amp;&amp; \\mathbf{v}^T\\mathbf{v}&amp;= \\mathbf{1} \\\\ &amp;&amp; \\mathbf{u}^T\\mathbf{u}&amp;= \\mathbf{1} \\\\ &amp;&amp; |\\mathbf{v}|&amp; \\le c_2 \\\\ \\end{aligned} \\tag{8.3} \\end{equation} \\] This looks like singular-value decomposition with LASSO penality on \\(\\mathbf{v}\\). Discussing the implementation of their proposed algorithm could be interesting, but is beyond the scope of this. Fortunately the authors have build an R package that conveniently implements their algorithm. Let’s generate a random matrix with \\(n=50\\) and \\(p=1000\\) and see if we can still apply the ideas developed above for feature extraction. It turns out we can proceed pretty much exactly as before. Once again we will pre-multiply by \\(\\mathbf{V}^2\\) to obtain a ranking in terms of feature importance. Here it should be noted that we would not generally compute all \\(p\\) principal components, but focus on say the first \\(l\\). Of course for \\(l&lt;p\\) they will never not explain the total variation in \\(X\\). In that sense our ranking vector \\(\\mathbf{r}\\) now ranks features in terms of their contribution to the overall variation explained by the first \\(l\\) sparse principal components. Of course for many features that contribution will be zero, since the LASSO penalty shrinks some elements in \\(\\mathbf{V}\\) to zero. Hence some selection is already done for use, but we can still proceed as before to select our final set of \\(m\\) features. feature importance 103 0.2217190 652 0.1580899 827 0.0993328 69 0.0881620 945 0.0815937 8.4.2 Fast, partial SVD 8.5 Forward search An interesting idea could be combine the above methods with forward search: Forward search As per the image above, at the \\(k\\)-th step forward search requires fitting the model \\((p-k)\\) times to compare scores. A good idea might be to use the ranking of features obtained from PCA and only fit the model for the \\(l&lt;(p-k)\\) most important features at each step. The below summarizes the idea: Run PCA to rank features in terms of their contributions to the overall variation in the design matrix \\(X\\). Run a forward search: In the first step fit the model for the \\(l\\) most important features (ranked by PCA). Choose the feature \\(x_j\\) with highest score and remove it from the ranking. Proceed in the same way in the following step until convergence. Note that convergence is somewhat loosely defined here. In practice I would look at how much the model improves by including an additional features (e.g. how much the likelihood increases or the MSE decreases). For that we would need to define some threshold value \\(\\tau\\) which would correspond to the minimum improvement we would want to obtain for including another feature (but strictly speaking that is a hyperparameter, so if you wanted to avoid those altogether I would just rely on the ranking methods proposed above). "],
["subsample.html", "Chapter 9 Subsampling 9.1 Motivation 9.2 Subsampling methods 9.3 Linear regression model 9.4 Classification problems 9.5 Conclusion 9.6 Appendix", " Chapter 9 Subsampling When working with very large sample data, even the estimation of ordinary least-squares can be computationally prohibitive. Since we increasingly find ourselves in situations where the sample size \\(n\\) is extremely high, a body of literature concerned with optimal subsampling has recently emerged. This post summarises some of the main ideas and methodologies that have emerged from that literature. This chapter is structured as follows: the first section briefly revisits the bias-variance trade-off already introduced in Chapter 7.1. The following section then introduces various subsampling methods. Section 9.3 illustrates the improvements associated with non-uniform subsampling. The final two sections apply the developed ideas to binary classification problems with imbalanced training data. Section 9.5 concludes. 9.1 Motivation Recall the discussion of bias-variance trade-off in Chapter 7.1 where we showed that as model complexity increases the variance component of the mean-squared error increases, while the bias term diminishes. The focus here is instead on varying the sample size \\(n\\). It should not be surprising that both the variance and bias component of the MSE decrease as the sample size \\(n\\) increases (Figure 9.1). But in today’s world \\(n\\) can potentially be very large, so much so that even computing simple linear models can be hard. Suppose for example you wanted to use patient data that is generated in real-time as a global pandemic unfolds to predict the trajectory of said pandemic. Or consider the vast quantities of potentially useful user-generated data that online service providers have access to. In the remainder of this note we will investigate how systematic subsampling can help improve model accuracy in these situations. Figure 9.1: Bias-variance trade-off 9.2 Subsampling methods The case for subsampling generally involves \\(n &gt;&gt; p\\), so very large values of \\(n\\). In such cases we may be interested in estimating \\(\\hat\\beta_m\\) instead of \\(\\hat\\beta_n\\) where \\(p\\le m&lt;&lt;n\\) with \\(m\\) freely chosen by us. In practice we may want to do this to avoid high computational costs associated with large \\(n\\) as discussed above. The basic algorithm for estimating \\(\\hat\\beta_m\\) is simple: Subsample with replacement from the data with some sampling probability \\(\\{\\pi_i\\}^n_{i=1}\\). Estimate least-squares estimator \\(\\hat\\beta_m\\) using the subsample. But there are at least two questions about this algorithm: firstly, how do we choose \\(\\mathbf{X}_m=({\\mathbf{X}^{(1)}}^T,...,{\\mathbf{X}^{(m)}}^T)^T\\)? Secondly, how should we construct \\(\\hat\\beta_m\\)? With respect to the former, a better idea than just randomly selecting \\(\\mathbf{X}_m\\) might be to choose observations with high influence. We will look at a few of the different subsampling methods investigated and proposed in Zhu et al. (2015), which differ primarily in their choice of subsampling probabilities \\(\\{\\pi_i\\}^n_{i=1}\\): Uniform subsampling (UNIF): \\(\\{\\pi_i\\}^n_{i=1}=1/n\\). Basic leveraging (BLEV): \\(\\{\\pi_i\\}^n_{i=1}=h_{ii}/ \\text{tr}(\\mathbf{H})=h_{ii}/p\\) where \\(\\mathbf{H}\\) is the hat matrix. Optimal (OPT) and predictor-length sampling (PL): involving \\(||\\mathbf{X}_i||/ \\sum_{j=1}^{n}||\\mathbf{X}_j||\\) where \\(||\\mathbf{X}||\\) denotes the \\(L_2\\) norm of \\(\\mathbf{X}\\). Methods involving predictor-lengths are proposed by the authors with the former shown to be optimal (more on this below). PL subsampling is shown to scale very well and a good approximation of optimal subsampling conditional on leverage scores \\(h_{ii}\\) being fairly homogeneous. With respect to the second question Zhu et al. (2015) investigate both ordinary least-squares (OLS) and weighted least-squares (WLS). With respect to the latter the weights we need to supply to the ols function introduced in Chapter 4.1 are simply \\(w=\\{1/\\pi_i\\}^m_{i=1}\\). This follows from the following property of diagonal matrices: \\[ \\begin{aligned} &amp;&amp; \\Phi^{-1}&amp;= \\begin{pmatrix} 1\\over\\phi_{11} &amp; 0 &amp; 0 \\\\ 0 &amp; ... &amp; 0 \\\\ 0 &amp; 0 &amp; 1\\over \\phi_{nn} \\end{pmatrix} \\\\ \\end{aligned} \\] The authors present empirical evidence that OLS is more efficient than WLS in that the mean-squared error (MSE) for predicting \\(\\mathbf{X} \\beta\\) is lower for OLS. The authors also note though that subsampling using OLS is not consistent for non-uniform subsampling methods meaning that the bias cannot be controlled. Given Equation (2.1) the fact that OLS is nonetheless more efficient than WLS implies that the higher variance terms associated with WLS dominates the effect of relatively higher bias with OLS. In fact this is consistent with the theoretical results presented in Zhu et al. (2015) (more on this below). Next we will briefly run through different estimation and subsampling methods in some more detail and see how they can be implemented in R. In the following section we will then look at how the different approaches perform empirically. 9.2.1 Uniform subsampling (UNIF) A simple function for uniform subsampling in R is shown in the code chunk below. Note that to streamline the comparison of the different methods in the following section the function takes an unused argument weighted=F which for the other subsampling methods can be used to determine whether OLS or WLS should be used. Of course, with uniform subsampling the weights are all identical and hence \\(\\hat\\beta^{OLS}=\\hat\\beta^{WLS}\\) so the argument is passed to but not evaluated in sub_UNIF. function (X, y, m, weighted = F, rand_state = NULL) { if (!is.null(rand_state)) { set.seed(rand_state) } indices &lt;- sample(1:n, size = m) X_m &lt;- X[indices, ] y_m &lt;- y[indices] beta_hat &lt;- qr.solve(X_m, y_m) y_hat &lt;- c(X %*% beta_hat) return(list(fitted = y_hat, coeff = beta_hat)) } 9.2.2 Basic leveraging (BLEV) The sub_UNIF function can be extended easily to the case with basic leveraging (see code below). Note that in this case the weighted argument is evaluated. function (X, y, m, weighted = F, rand_state = NULL, plot_wgts = F, prob_only = F) { svd_X &lt;- svd(X) U &lt;- svd_X$u H &lt;- tcrossprod(U) h &lt;- diag(H) prob &lt;- h/ncol(X) if (plot_wgts) { plot(prob, t = &quot;l&quot;, ylab = &quot;Sampling probability&quot;) } if (prob_only) { return(prob) } else { indices &lt;- sample(x = 1:n, size = m, replace = T, prob = prob) X_m &lt;- X[indices, ] y_m &lt;- y[indices] weights &lt;- 1/prob[indices] if (weighted) { beta_hat &lt;- wls_qr(X_m, y_m, weights) } else { beta_hat &lt;- qr.solve(X_m, y_m) } y_hat &lt;- c(X %*% beta_hat) return(list(fitted = y_hat, coeff = beta_hat, prob = prob)) } } 9.2.2.1 A note on computing leverage scores Recall that for the hat matrix we have \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\mathbf{H}&amp;=\\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T \\\\ \\end{aligned} \\tag{9.1} \\end{equation} \\] where the diagonal elements \\(h_{ii}\\) correspond to the leverage scores we’re after. Following Zhu et al. (2015) we will use (compact) singular value decomposition to obtain \\(\\mathbf{H}\\) rather than computing (9.1) directly. This has the benefit that there exist exceptionally stable numerical algorithms to compute SVD. To see how and why we can use SVD to obtain \\(\\mathbf{H}\\) see appendix. Clearly to get \\(h_{ii}\\) we first need to compute \\(\\mathbf{H}\\) which in terms of computational costs is of order \\(\\mathcal{O}(np^2)=\\max(\\mathcal{O}(np^2),\\mathcal{O}(p^3))\\). The fact that we use all \\(n\\) rows of \\(\\Phi\\) to compute leverage scores even though we explicitly stated our goal was to only use \\(m\\) observations may rightly seem like a bit of a paradox. This is why fast algorithms that approximate leverage scores have been proposed. We will not look at them specifically here mainly because the PL method proposed by Zhu et al. (2015) does not depend on leverage scores and promises to be computationally even more efficient. 9.2.3 Predictor-length sampling (PL) The basic characteristic of PL subsampling - choosing \\(\\{\\pi_i\\}^n_{i=1}= ||\\mathbf{X}_i||/ \\sum_{j=1}^{n}||\\mathbf{X}_j||\\) - was already introduced above. Again it is very easy to modify the subsampling functions from above to this case: function (X, y, m, weighted = F, rand_state = NULL, plot_wgts = F, prob_only = F) { predictor_len &lt;- sqrt(X^2 %*% rep(1, ncol(X))) prob &lt;- predictor_len/sum(predictor_len) if (plot_wgts) { plot(prob, t = &quot;l&quot;, ylab = &quot;Sampling probability&quot;) } if (prob_only) { return(prob) } else { indices &lt;- sample(x = 1:n, size = m, replace = T, prob = prob) X_m &lt;- X[indices, ] y_m &lt;- y[indices] weights &lt;- 1/prob[indices] if (weighted) { beta_hat &lt;- wls_qr(X_m, y_m, weights) } else { beta_hat &lt;- qr.solve(X_m, y_m) } y_hat &lt;- c(X %*% beta_hat) return(list(fitted = y_hat, coeff = beta_hat, prob = prob)) } } 9.2.3.1 A note on optimal subsampling (OPT) In fact, PL subsampling is an approximate version of optimal subsampling (OPT). Zhu et al. (2015) show that asymptotically we have: \\[ \\begin{equation} \\begin{aligned} &amp;&amp;\\text{plim} \\left( \\text{var} (\\hat{f}_n(x)) \\right) &gt; \\text{plim} \\left(\\left( \\mathbb{E} \\left( \\hat{f}_n(x) \\right) - f(x) \\right)^2 \\right) \\\\ \\end{aligned} \\tag{9.2} \\end{equation} \\] Given this result minimizing the MSE (Equation (2.1)) with respect to subsampling probabilities \\(\\{\\pi_i\\}^n_{i=1}\\) corresponds to minimizing \\(\\text{var} (\\hat{f}_n(x))\\). They further show that this minimization problem has the following closed-form solution: \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\pi_i&amp;= \\frac{\\sqrt{(1-h_{ii})}||\\mathbf{X}_i||}{\\sum_{j=1}^n\\sqrt{(1-h_{jj})}||\\mathbf{X}_j||}\\\\ \\end{aligned} \\tag{9.3} \\end{equation} \\] This still has computational costs of order \\(\\mathcal{O}(np^2)\\). But it should now be clear why PL subsampling is optimal conditional on leverage scores being homogeneous (see appendix). PL subsampling is associated with computational costs of order \\(\\mathcal{O}(np)\\), so a potentially massive improvement. The code for optimal subsampling is shown below: function (X, y, m, weighted = F, rand_state = NULL, plot_wgts = F, prob_only = F) { n &lt;- nrow(X) svd_X &lt;- svd(X) U &lt;- svd_X$u H &lt;- tcrossprod(U) h &lt;- diag(H) predictor_len &lt;- sqrt(X^2 %*% rep(1, ncol(X))) prob &lt;- (sqrt(1 - h) * predictor_len)/crossprod(sqrt(1 - h), predictor_len)[1] if (plot_wgts) { plot(prob, t = &quot;l&quot;, ylab = &quot;Sampling probability&quot;) } if (prob_only) { return(prob) } else { indices &lt;- sample(x = 1:n, size = m, replace = T, prob = prob) X_m &lt;- X[indices, ] y_m &lt;- y[indices] weights &lt;- 1/prob[indices] if (weighted) { beta_hat &lt;- wls_qr(X_m, y_m, weights) } else { beta_hat &lt;- qr.solve(X_m, y_m) } y_hat &lt;- c(X %*% beta_hat) return(list(fitted = y_hat, coeff = beta_hat, prob = prob)) } } 9.2.3.2 A note on computing predictor lengths Computing the Euclidean norms \\(||\\mathbf{X}_i||\\) in R can be done explicitly by looping over the rows of \\(\\mathbf{X}\\) and computing the norm in each iteration. It turns out that this computationally very expensive. A much more efficient way of computing the vector of predictor lengths is as follows \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\mathbf{pl}&amp;=\\sqrt{\\mathbf{X}^2 \\mathbf{1}} \\\\ \\end{aligned} \\tag{9.4} \\end{equation} \\] where \\(\\mathbf{X}^2\\) indicates elements squared, the square root is also taken element-wise and \\(\\mathbf{1}\\) is a \\((p \\times 1)\\) vectors of ones. A performance benchmark of the two approaches is shown in Figure 9.2 below. Figure 9.2: Benchmark of Euclidean norm computations. 9.2.4 Comparison of methods As discussed in Zhu et al. (2015) both OPT and PL subsampling tend to inflate subsampling probabilities of observations with low leverage scores and shrink those of high-leverage observations relative to BLEV. They show explicitly that this always holds for orthogonal design matrices. As a quick sense-check of the functions introduced above we can generate a random orthogonal design matrix \\(\\mathbf{X}\\) and plot subsampling probabilities with OPT and PL against those obtained with BLEV. Figure 9.3 illustrates this relationship nicely. The design matrix \\(\\mathbf{X}\\) \\((n \\times p)\\) with \\(n=1000\\) and \\(p=100\\) was generated using SVD: list(function (n, p) { M &lt;- matrix(rnorm(n * p), n, p) X &lt;- svd(M)$u return(X) }) ## [[1]] ## function (n, p) ## { ## M &lt;- matrix(rnorm(n * p), n, p) ## X &lt;- svd(M)$u ## return(X) ## } Figure 9.3: Comparison of subsampling probabilities. 9.3 Linear regression model 9.3.1 A review of Zhu et al. (2015) To illustrate the improvements associated with the methods proposed in Zhu et al. (2015), we will briefly replicate their main empirical findings here. The evaluate the performance of the different methods we will proceed as follows: Empirical exercise Generate synthetic data \\(\\mathbf{X}\\) of dimension \\((n \\times m)\\) with \\(n&gt;&gt;m\\). Set some true model parameter \\(\\beta=(\\mathbf{1}^T_{\\overline{m*0.6}},\\mathbf{1}^T_{\\underline{m*0.4}})^T\\). Model the outcome variable as \\(\\mathbf{y}=\\mathbf{X}\\beta+\\epsilon\\) where \\(\\epsilon \\sim \\mathcal{N}(\\mathbf{0},\\sigma^2 \\mathbf{I}_n)\\) and \\(\\sigma=10\\). Estimate the full-sample OLS estimator \\(\\hat\\beta_n\\) (a benchmark estimator of sorts in this setting). Use one of the subsampling methods to estimate iteratively \\(\\{\\hat\\beta^{(b)}_m\\}^B_{b=1}\\). Note that all subsampling methods are stochastic so \\(\\hat\\beta_m\\) varies across iterations. Evaluate average model performance of \\(\\hat\\beta_m\\) under the mean-squared error criterium: \\(MSE= \\frac{1}{B} \\sum_{b=1}^{B} MSE^{(b)}\\) where \\(MSE^{(b)}\\) corresponds to the in-sample estimator of the mean-squared error of the \\(b\\)-th iteration. This exercise - and the once that follow - are computationally expensive. For them to be feasible we have to refrain from bias-correcting the in-sample estimator of the MSE (see appendix for session info). We will use a simple wrapper function to implement the empirical exercise in R. As in Zhu et al. (2015) we will generate the design matrix \\(\\mathbf{X}\\) from 5 different distributions: 1) Gaussian (GA) with \\(\\mathcal{N}(\\mathbf{0},\\Sigma)\\); 2) Mixed-Gaussian (MG) with \\(0.5\\mathcal{N}(\\mathbf{0},\\Sigma)+0.5\\mathcal{N}(\\mathbf{0},25\\Sigma)\\); 3) Log-Gaussian (LN) with \\(\\log\\mathcal{N}(\\mathbf{0},\\Sigma)\\); 4) T-distribution with 1 degree of freedom (T1) and \\(\\Sigma\\); 5) T-distribution as in 4) but truncated at \\([-p,p]\\). All parameters are chosen in the same way as in Zhu et al. (2015) with exception of \\(n=1000\\) and \\(p=3\\), which are significantly smaller choices in order to decrease the computational costs. The corresponding densities of the 5 data sets are shown in Figure 9.11 in the appendix. We will run the empirical exercise for each data set and each subsampling method introduced above. Figure 9.4 shows logarithms of the sampling probabilities corresponding to the different subsampling methods (UNIF not shown for obvious reasons). The plots look very similar to the one in Zhu et al. (2015) and is shown here primarily to reassure ourselves that we have implemented their ideas correctly. One interesting observation is worth pointing out however: note how the distributions for OPT and PL have lower standard deviations compared to BLEV. This should not be altogether surprising since we already saw above that for orthogonal design matrices the former methods inflate small leverage scores while shrinking high scores. But it is interesting to see that the same appears to hold for design matrices that are explicitly not orthogonal given our choice of \\(\\Sigma\\). Figure 9.4: Sampling probabilities for different subsampling methods. Figures 9.5 and 9.6 show the resulting MSE, squared bias and variance for the different subsampling methods and data sets using weighed least-squares and ordinary least-squares, respectively. The subsampling size increases along the horizontal axis. The figures are interactive to allow readers to zoom in etc. For the data sets that are also shown in Zhu et al. (2015) we find the same overall pattern: PL and OPT outperform other methods when using weighted least-squares, while BLEV outperforms other methods when using unweighted/ordinary least-squares. For Gaussian data (GA) the differences between the methods are minimal since data points are homogeneous. A similar picture emerges when running the method comparison for the sinusoidal data introduced above (see appendix). In fact, Zhu et al. (2015) recommend to just rely on uniform subsampling when data is Gaussian. Another interesting observation is that for t-distributed data (T1) the non-uniform subsampling methods significantly outperform uniform subsampling methods. This is despite the fact that in the case of T1 data the conditions used to establish asymptotic consistency of the non-uniform subsampling methods in Zhu et al. (2015) are not fulfilled: in particular the fourth moment is not finite (in fact it is not defined). Figure 9.5: MSE, squared bias and variance for different subsampling methods and data sets. Subsampling with weighted least-squares. Figure 9.6: MSE, squared bias and variance for different subsampling methods and data sets. Subsampling with ordinary least-squares. 9.3.2 Computational performance We have already seen above that theoretically speaking both BLEV and OPT subsampling are computationally more expensive than PL subsampling (with UNIF subsampling the least expensive). It should be obvious that in light of their computational costs \\(\\mathcal{O}(np^2)\\) the former two methods do not scale well in higher-dimensional problems (higher \\(p\\)). Zhu et al. (2015) demonstrate this through empirical exercises to an extent that is beyond the scope of this note. Instead we will just quickly benchmark the different functions for non-uniform subsampling introduced above: sub_BLEV, sub_OPT, sub_PL for \\(n=200\\), \\(p=100\\). We are only interested in how long it takes to compute subsampling probabilities, and since for sub_UNIF all subsampling probabilities are simply \\(\\pi_i=1/n\\) we neglect this here. Figure 9.7 benchmarks the three non-uniform subsampling methods. Evidently PL subsampling is computationally much less costly. Figure 9.7: Benchmark of computational performance of different methods. 9.4 Classification problems In binary classification problems we are often faced with the issue of imbalanced training data - one of the two classes is under-represented relative to the other. This generally makes classifiers less sensitive to the minority class which often is the class we want to predict (Branco, Torgo, and Ribeiro (2015)). Suppose for example we wanted to predict the probability of death for patients who suffer from COVID-19. The case-fatality rate for the virus is significantly lower than 10% so any data we could obtain on patients would inevitably be imbalanced: the domain is skewed towards the class we are not interested in predicting. A common and straight-forward way to deal with this issue is to randomly over- or under-sample the training data. Let \\(y_i\\in{0,1}\\) for all \\(i=1,...,n\\). We are interested in modelling \\(p_i=P(y_i=1|x_i)\\) but our data is imbalanced: \\(n_{y=0}&gt;&gt;n_{y=1}\\) where \\(n=n_{y=0}+n_{y=1}\\). Then random over- and under-sampling works as follows: Random oversampling: draw \\(y_i\\) from minority class \\(\\mathbf{y}_{n_{y=1}}\\) with probability \\(\\{\\pi_i\\}^{n_{y=1}}_{i=1}=1/n_{y=1}\\) and append \\(\\mathbf{y}_{n_{y=1}}\\) by \\(y_i\\) so that \\(n_{y=1} \\leftarrow n_{y=1}+1\\) until \\(n_{y=0}=n_{y=1}\\). Random undersampling: draw \\(y_i\\) from majority class \\(\\mathbf{y}_{n_{y=0}}\\) with probability \\(\\{\\pi_i\\}^{n_{y=0}}_{i=0}=1/n_{y=0}\\) and remove \\(y_i\\) from \\(\\mathbf{y}_{n_{y=0}}\\) so that \\(n_{y=0} \\leftarrow n_{y=0}-1\\) until \\(n_{y=0}=n_{y=1}\\). In a way both these methods correspond to uniform subsampling (UNIF) discussed above. Random oversampling may lead to overfitting. Conversely, random undersampling may come at the cost of eliminating observations with valuable information. With respect to the latter, we have already shown that more systematic subsampling approaches generally outperform uniform subsampling in linear regression problems. It would be interesting to see if we can apply similar ideas to classification with imbalanced data. How exactly we can go about doing this should be straight-forward to see: Systematic undersampling: draw \\(n_{y=1}\\) times from from majority class \\(\\mathbf{y}_{n_{y=0}}\\) with probability \\(\\{\\pi_i\\}^{n_{y=0}}_{i=0}\\) defined by optimal subsampling methods and throw out the remainder. To remain in the subsampling framework we will focus on undersampling here. It should be noted that many systematic approaches to undersampling already exist. In their extensive survey Branco, Torgo, and Ribeiro (2015) mention undersampling methods based on distance-criteria, condensed nearest neighbours as well as active learning methods. Optimal subsampling is not mentioned in the survey. 9.4.1 Optimal subsampling for classification problems We cannot apply the methods used for subsampling with linear regression directly to classification problems, although we will see that certain ideas still apply. Wang, Zhu, and Ma (2018) explore optimal subsampling for large sample logistic regression - a paper that is very much related to Zhu et al. (2015). We will not cover the details here as much as we did for Zhu et al. (2015) and instead jump straight to one of the main results. Wang, Zhu, and Ma (2018) and propose a two-step algorithm that in the first step estimates logistic regression with uniform subsampling using \\(m_0\\) observations. In the second step the estimated coefficient \\(\\beta^{(UNIF)}_{m_0}\\) from the first step is used to compute subsampling probabilities as follows: \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\pi_i&amp;= \\frac{|y_i-p_i(\\beta^{(UNIF)}_{m_0})|||\\mathbf{X}_i||}{\\sum_{j=1}^n|y_i-p_i(\\beta^{(UNIF)}_{m_0})|||\\mathbf{X}_j||}\\\\ \\end{aligned} (\\#eq:subs_mVc) \\end{equation} \\] The probabilities are then used to subsample \\(m\\) observations again. Finally, \\(\\hat\\beta_{m_0+m}\\) is estimated from the combdined subsamples with weighted logistic regression where weights once again correspond to inverse sampling probabilities. The authors refer to the method corresponding to @ref{eq:subs_mVc} as mVc since it corresponds to optimal subsampling with variance targeting. They also propose a method which targets the MSE directly, but this one is of higher computational costs \\(\\mathcal{O}(np^2)\\) vs. \\(\\mathcal{O}(np)\\) for mVc. The similarities to the case for linear regression should be obvious. We will only consider mVc here and see if it can be applied to undersampling. The code that implements this can be found here - it uses object-oriented program so could be interesting for some readers. Wang, Zhu, and Ma (2018) never run the model with ordinary as opposed to weighted logit, possibly because ordinary logit is not sensible in this context. But for the sake of completeness we’ll once again consider both cases here. 9.4.2 Synthetic data In this section we will use the same synthetic data sets as above for our design matrix \\(\\mathbf{X}\\). But while above we sampled \\(y_i |x_i\\sim \\mathcal{N}(\\beta^T x_i,\\sigma)\\), here we will simulate a single draw from \\(\\mathbf{y} |\\mathbf{X} \\sim \\text{Bernoulli}(p)\\), where in order to create imbalance we let \\(p&lt;0.5\\) vary. When thinking about repeating the empirical exercise from Section 9.3 above, we are faced with the question of how to compute the MSE and its bias-variance decomposition. One idea could be to compare the linear predictions in (5.9) in the same way as we did before, since after all these are fitted values from a linear model. The problem with that idea is that rebalancing the data through undersampling affects the intercept term \\(\\beta_0\\) (potentially quite a bit if the data was originally very imbalanced). This is not surprising since \\(\\beta_0\\) measures the log odds of \\(y_i\\) given all predictors \\(\\mathbf{X}_i=0\\) are zero. Hence comparing linear predictors \\(\\mathbf{X}\\beta_n\\) and \\(\\mathbf{X}\\beta_m\\) is not a good idea in this setting (perhaps ever?). Fortunately though we can just work with the predicted probabilities in (5.8) directly and decompose the MSE in the same way as before (see Manning, Schütze, and Raghavan (2008), pp. 310): \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\mathbb{E} \\left( (\\hat{\\mathbf{p}}-\\mathbf{p})^2 \\right) &amp;= \\text{var} (\\hat{\\mathbf{p}}) + \\left( \\mathbb{E} \\left( \\hat{\\mathbf{p}} \\right) - \\mathbf{p} \\right)^2 \\\\ \\end{aligned} \\tag{9.5} \\end{equation} \\] Hence the empirical exercise in this section is very similar to the one above and can be summarized as follows: Empirical exercise Generate synthetic data \\(\\mathbf{X}\\) of dimension \\((n \\times m)\\) with \\(n&gt;&gt;m\\). Randomly generate the binary outcome variable as \\(\\mathbf{y} | \\mathbf{X} \\sim \\text{Bernoulli}(p=m/n)\\). Estimate the full-sample logit estimator \\(\\hat\\beta_n\\). Use one of the undersampling methods to estimate recursively \\(\\{\\hat\\beta^{(b)}_m\\}^B_{b=1}\\). Evaluate average model performance of \\(\\hat\\beta_m\\) under the mean-squared error criterium: \\(MSE= \\frac{1}{B} \\sum_{b=1}^{B} MSE^{(b)}\\) where \\(MSE^{(b)}\\) corresponds to the in-sample estimator of the mean-squared error of the \\(b\\)-th iteration. There is a decisive difference between this exercise and the one we ran for linear regression models above: here we are mechanically reducing the number of observations of the majority class and with non-uniform subsampling we are selective about what observations we keep. This is very different to simply subsampling without imposing a structure on the subsample. Intuitively it may well be that in this context non-uniform subsampling is prone to produce estimates of \\(\\hat\\beta_m\\) that are very different from \\(\\hat\\beta_n\\). This effect could be expected to appear in the squared bias term of the MSE. The resulting mean-squared error decompositions are shown in Figures 9.8 and 9.9 for weighted and ordinary logit. A first interesting and reassuring observation is that both for weighted and ordinary logit the variance component is generally reduced more with mVc subsampling (certainly less true for weighted logit). A second observation is that the squared bias term can evidently not be controlled with mVc which leads to the overall mean-squared errors generally being higher than with random undersampling. This could be evidence that selective subsampling in fact introduces bias as speculated above, but this is not clear and would have to be explored more thoroughly. It could also very well be that the algorithm in Wang, Zhu, and Ma (2018) was not implemented correctly here and the results are therefore incorrect. Unfortunately I have not had time to try and completely replicate that second paper also. Figure 9.8: MSE, squared bias and variance for different subsampling methods and data sets. Subsampling with weighted logistic regression. MSE and its components correspond to linear predictions of logistic regression. Figure 9.9: MSE, squared bias and variance for different subsampling methods and data sets. Subsampling with ordinary logistic regression. MSE and its components correspond to linear predictions of logistic regression. 9.4.3 Real data example Until now we have merely looked at approximations of \\(\\hat\\beta_n\\) for logistic regression with imbalanced data. But ultimately the goal in classification problems is to accurately predict \\(\\mathbf{y}\\) from test data. To investigate this point we will next turn to real data. A number of interesting data sets related to COVID-19 have been made publicly available for research. An interesting data set for our purposes that contains information about individual patients is being maintained for the Open COVID-19 Data Working Group. The methodology for building the data set was originally developed by was developed by Xu et al. (2020). Information about how the data is being maintained and updated can be found in Group (2020). The complete data set is vast, but for our purposes we are only interested in the subset of data that contains information about the patient’s outcome state. The subset of the data we will use for our prediction exercise here is shown below. It contains the patient outcome variable \\(y\\in \\{0=\\text{survived}, 1=\\text{deceased}\\}\\) which we are interested in predicting from a set of features: age, gender and geographical location. The model is kept intentionally simple, as our goal here continues to be the evaluation of subsampling methods, rather than building an accurate prediction. The outcome variable is defined as \\(y\\in\\{0=\\text{survived},1=\\text{deceased}\\}\\) and is highly imbalanced as expected: deaths make up about 4 percent of all outcomes. Note also that we are now looking at a significantly higher value for \\(n\\) than in the synthetic examples above. Computing leverage scores in this setting is extremely costly for reasons we have already elaborated on above and hence basic leveraging (BL) and optimal subsampling (OPT) are both omitted from the remainder of the analysis. To compare the two remaining approaches - uniform (UNIF) and prediction-length (PL) subsampling - we will proceed as follows: To score our probabilistic predictions we will once again use the mean-squared error, although technically in this setting we would speak of the Brier score (see here): \\[ \\begin{equation} \\begin{aligned} &amp;&amp; BS&amp;= \\frac{1}{n} \\sum_{i=1}^{n} (p_i-\\hat{p}_i)^2 \\\\ \\end{aligned} \\tag{9.6} \\end{equation} \\] Of course other scoring functions are commonly considered for classification problems (for a discussion see for example here). But using the Brier score naturally continues the discussion above. Empirical exercise Cross-validate with \\(k=5\\) folds and for each fold repeat \\(B=100\\) times: Undersample the training data through one of the methods: \\((\\mathbf{X}_{\\text{train}},\\mathbf{y}_{\\text{train}})\\). Fit the model on the undersampled training data. Using the fitted model predict \\(\\hat{\\mathbf{y}}_{\\text{train}}\\) using the test data \\(\\mathbf{X}_{\\text{test}}\\). Compute the Brier score for \\(\\mathbf{y}_{\\text{test}}\\) and \\(\\hat{\\mathbf{y}}_{\\text{train}}\\). Figure 9.10 shows the distribution of Brier scores resulting from our pseudo-out-of-sample predictions. Undersampling is performed both randomly and with mVc subsampling. The logit classifier is fitted separately with and without subsampling probabilities as weights. In the case of ordinary logit mVc seems to lead to slightly better predictions on average, but the improvement is not very significant. When the model is fitted with weights something interesting happens. The majority of the estimated Brier scores are lower than those obtained with random undersampling, but there is much higher variance and in some cases mVc performs very poorly. This could imply that using optimal subsampling for undersampling with weighted regression is risky: it performs well most of the time, but sometimes things go very wrong. Of course, this is a just a single example and the findings shown here are not sufficient to draw any conclusions. Figure 9.10: Distribution of Brier scores from pseudo-out-of-sample predictions for random and prediction-length undersampling. 9.5 Conclusion In this short note we have looked at various systematic subsampling methods and how they can be applied to imbalanced learning. The fact that non-uniform subsampling can improve performance of linear regression models has been well established (Zhu et al. (2015)). We reviewed these findings here and then applied the developed idea to a binary classification problem with imbalanced data. The findings are mixed: it does appear that optimal subsampling methods can be successfully applied to undersampling in the sense that they can improve modely accuracy. While these findings are potentially interesting they are not conclusive. To establish general results theory would need to be developed. It is also far from clear whether the undersampling approaches presented here can compete with existing methodologies that involve distance-criteria, condensed nearest neighbours and active learning methods. 9.6 Appendix 9.6.1 From SVD to leverage scores From SVD to leverage 9.6.2 From optimal to prediction-length subsampling From OPT to PL 9.6.3 Synthetic data Figure 9.11: Densities of synthetic design matrices. 9.6.4 Subsampling applied to sinusoidal function "],
["outl.html", "Chapter 10 Outliers 10.1 Trimmed mean estimator 10.2 Median-of-means estimator", " Chapter 10 Outliers The empirical mean is sensitive to outliers. 10.1 Trimmed mean estimator One way of dealing with outliers is to simply remove them. With respect to empirical mean estimation this corresponding estimator is referred to as trimmed mean estimator, \\(m_n^{(k)}\\). It simply ignores the top and bottom \\(k\\) values. We have \\[ \\begin{equation} \\begin{aligned} &amp;&amp; \\mathbb{E}m_n^{(k)}&amp;= \\mathbb{E} \\frac{1}{n-2k} \\sum_{i=1}^{n} \\mathbb{1}_{x_i \\notin \\text{top/bottom}} x_i\\\\ \\end{aligned} \\tag{10.1} \\end{equation} \\] One can show that if \\(k \\approx \\log( \\frac{1}{\\delta})\\), then with probability \\[ \\begin{aligned} &amp;&amp; |m_n-m|&amp;=c \\sqrt{ \\frac{\\delta^2 \\log( \\frac{1}{\\delta})}{n}} \\\\ \\end{aligned} \\] 10.2 Median-of-means estimator Another idea involves repeatedly estimating the empirical means of subsets of the data and taking the median of those. In particular, divide the data into \\(k\\) blocks of \\(l\\) points each. For each block compute \\(m_n^{(j)}= \\frac{1}{l}\\sum_{j=1}^{l}x_i\\). Then the median-of-means estimator is simply: \\[ \\begin{equation} \\begin{aligned} &amp;&amp; m_n&amp;=\\text{median}(m_n^{(1)},...,m_n^{(k)}) \\\\ \\end{aligned} \\tag{10.2} \\end{equation} \\] "],
["references.html", "References", " References Bishop, Christopher M. 2006. Pattern Recognition and Machine Learning. springer. Branco, Paula, Luis Torgo, and Rita Ribeiro. 2015. “A Survey of Predictive Modelling Under Imbalanced Distributions.” arXiv Preprint arXiv:1505.01658. Group, Open COVID-19 Data Working. 2020. “Detailed Epidemiological Data from the COVID-19 Outbreak.” Accessed on yyyy-mm-dd from http://virological.org/t/epidemiological-data-from-the-ncov-2019-outbreak-early-descriptions-from-publicly-available-data/337. Jolliffe, Ian T, Nickolay T Trendafilov, and Mudassir Uddin. 2003. “A Modified Principal Component Technique Based on the LASSO.” Journal of Computational and Graphical Statistics 12 (3): 531–47. Manning, Christopher D, Hinrich Schütze, and Prabhakar Raghavan. 2008. Introduction to Information Retrieval. Cambridge university press. Nocedal, Jorge, and Stephen Wright. 2006. Numerical Optimization. Springer Science &amp; Business Media. Wang, HaiYing, Rong Zhu, and Ping Ma. 2018. “Optimal Subsampling for Large Sample Logistic Regression.” Journal of the American Statistical Association 113 (522): 829–44. Wasserman, Larry. 2013. All of Statistics: A Concise Course in Statistical Inference. Springer Science &amp; Business Media. Witten, Daniela M, Robert Tibshirani, and Trevor Hastie. 2009. “A Penalized Matrix Decomposition, with Applications to Sparse Principal Components and Canonical Correlation Analysis.” Biostatistics 10 (3): 515–34. Xu, Bo, Bernardo Gutierrez, Sumiko Mekaru, Kara Sewalk, Lauren Goodwin, Alyssa Loskill, Emily Cohn, et al. 2020. “Epidemiological data from the COVID-19 outbreak, real-time case information.” Scientific Data 7 (106). https://doi.org/doi.org/10.1038/s41597-020-0448-0. Zhu, Rong, Ping Ma, Michael W Mahoney, and Bin Yu. 2015. “Optimal Subsampling Approaches for Large Sample Linear Regression.” arXiv, arXiv–1509. "]
]
